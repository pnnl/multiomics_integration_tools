{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Python version 3.11.7\n",
    "\n",
    "# Load general use packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load torch for making tensors\n",
    "import torch\n",
    "\n",
    "# Local scripts\n",
    "from multi_mlp import simple_FC, JointMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inputs\n",
    "\n",
    "Here, we will define one model per omic. Hidden layers can be set and should account for data size. DeepIMV cannot handle missing values, so a dataset with missing values removed and/or imputed is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16S has 8 features\n",
      "Metaproteomics has 2726 features\n",
      "Metabolomics Positive has 2800 features\n",
      "Metabolomics Negative has 1752 features\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "rna_16s = pd.read_csv(\"../../Dataset/Scaled/16S_Edata.csv\").drop(\"Feature\", axis = 1)\n",
    "print(\"16S has \" + str(len(rna_16s)) + \" features\")\n",
    "\n",
    "metaproteomics = pd.read_csv(\"../../Dataset/Scaled/Metaproteomics.csv\").drop(\"Feature\", axis = 1)\n",
    "print(\"Metaproteomics has \" + str(len(metaproteomics)) + \" features\")\n",
    "\n",
    "metabpos = pd.read_csv(\"../../Dataset/Scaled/Metabolomics_Positive.csv\").drop(\"Feature\", axis = 1)\n",
    "print(\"Metabolomics Positive has \" + str(len(metabpos)) + \" features\")\n",
    "\n",
    "metabneg = pd.read_csv(\"../../Dataset/Scaled/Metabolomics_Negative.csv\").drop(\"Feature\", axis = 1)\n",
    "print(\"Metabolomics Negative has \" + str(len(metabneg)) + \" features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Here we will test different grid sizes: \n",
    "\n",
    "| Approximate Proportional Size of Datasets | 16S Size | Metaproteomics & Metabolomics Sizes |\n",
    "|---|---|---|\n",
    "| 1/4 | 2 | 512 |\n",
    "| 1/2 | 4 | 1024 |\n",
    "| 1 | 8 | 2048 |\n",
    "| 2 | 16 | 4096 | \n",
    "| 4 | 32 | 8192 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element Size Test: 0 and Crossfold Number: 0\n",
      "Element Size Test: 0 and Crossfold Number: 1\n",
      "Element Size Test: 0 and Crossfold Number: 2\n",
      "Element Size Test: 0 and Crossfold Number: 3\n",
      "Element Size Test: 0 and Crossfold Number: 4\n",
      "Element Size Test: 0 and Crossfold Number: 5\n",
      "Element Size Test: 0 and Crossfold Number: 6\n",
      "Element Size Test: 0 and Crossfold Number: 7\n",
      "Element Size Test: 0 and Crossfold Number: 8\n",
      "Element Size Test: 0 and Crossfold Number: 9\n",
      "Element Size Test: 0 and Crossfold Number: 10\n",
      "Element Size Test: 0 and Crossfold Number: 11\n",
      "Element Size Test: 0 and Crossfold Number: 12\n",
      "Element Size Test: 0 and Crossfold Number: 13\n",
      "Element Size Test: 0 and Crossfold Number: 14\n",
      "Element Size Test: 1 and Crossfold Number: 0\n",
      "Element Size Test: 1 and Crossfold Number: 1\n",
      "Element Size Test: 1 and Crossfold Number: 2\n",
      "Element Size Test: 1 and Crossfold Number: 3\n",
      "Element Size Test: 1 and Crossfold Number: 4\n",
      "Element Size Test: 1 and Crossfold Number: 5\n",
      "Element Size Test: 1 and Crossfold Number: 6\n",
      "Element Size Test: 1 and Crossfold Number: 7\n",
      "Element Size Test: 1 and Crossfold Number: 8\n",
      "Element Size Test: 1 and Crossfold Number: 9\n",
      "Element Size Test: 1 and Crossfold Number: 10\n",
      "Element Size Test: 1 and Crossfold Number: 11\n",
      "Element Size Test: 1 and Crossfold Number: 12\n",
      "Element Size Test: 1 and Crossfold Number: 13\n",
      "Element Size Test: 1 and Crossfold Number: 14\n",
      "Element Size Test: 2 and Crossfold Number: 0\n",
      "Element Size Test: 2 and Crossfold Number: 1\n",
      "Element Size Test: 2 and Crossfold Number: 2\n",
      "Element Size Test: 2 and Crossfold Number: 3\n",
      "Element Size Test: 2 and Crossfold Number: 4\n",
      "Element Size Test: 2 and Crossfold Number: 5\n",
      "Element Size Test: 2 and Crossfold Number: 6\n",
      "Element Size Test: 2 and Crossfold Number: 7\n",
      "Element Size Test: 2 and Crossfold Number: 8\n",
      "Element Size Test: 2 and Crossfold Number: 9\n",
      "Element Size Test: 2 and Crossfold Number: 10\n",
      "Element Size Test: 2 and Crossfold Number: 11\n",
      "Element Size Test: 2 and Crossfold Number: 12\n",
      "Element Size Test: 2 and Crossfold Number: 13\n",
      "Element Size Test: 2 and Crossfold Number: 14\n",
      "Element Size Test: 3 and Crossfold Number: 0\n",
      "Element Size Test: 3 and Crossfold Number: 1\n",
      "Element Size Test: 3 and Crossfold Number: 2\n",
      "Element Size Test: 3 and Crossfold Number: 3\n",
      "Element Size Test: 3 and Crossfold Number: 4\n",
      "Element Size Test: 3 and Crossfold Number: 5\n",
      "Element Size Test: 3 and Crossfold Number: 6\n",
      "Element Size Test: 3 and Crossfold Number: 7\n",
      "Element Size Test: 3 and Crossfold Number: 8\n",
      "Element Size Test: 3 and Crossfold Number: 9\n",
      "Element Size Test: 3 and Crossfold Number: 10\n",
      "Element Size Test: 3 and Crossfold Number: 11\n",
      "Element Size Test: 3 and Crossfold Number: 12\n",
      "Element Size Test: 3 and Crossfold Number: 13\n",
      "Element Size Test: 3 and Crossfold Number: 14\n",
      "Element Size Test: 4 and Crossfold Number: 0\n",
      "Element Size Test: 4 and Crossfold Number: 1\n",
      "Element Size Test: 4 and Crossfold Number: 2\n",
      "Element Size Test: 4 and Crossfold Number: 3\n",
      "Element Size Test: 4 and Crossfold Number: 4\n",
      "Element Size Test: 4 and Crossfold Number: 5\n",
      "Element Size Test: 4 and Crossfold Number: 6\n",
      "Element Size Test: 4 and Crossfold Number: 7\n",
      "Element Size Test: 4 and Crossfold Number: 8\n",
      "Element Size Test: 4 and Crossfold Number: 9\n",
      "Element Size Test: 4 and Crossfold Number: 10\n",
      "Element Size Test: 4 and Crossfold Number: 11\n",
      "Element Size Test: 4 and Crossfold Number: 12\n",
      "Element Size Test: 4 and Crossfold Number: 13\n",
      "Element Size Test: 4 and Crossfold Number: 14\n"
     ]
    }
   ],
   "source": [
    "# Define sizes\n",
    "small_size = [2, 4, 8, 16, 32]\n",
    "large_size = [512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "# Load splits \n",
    "splits = []\n",
    "with open(\"splits.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        values = line.replace(\"\\n\", \"\").split(\" \")\n",
    "        values = [int(x)-1 for x in values] # Correct for indexing changes between R and python\n",
    "        splits.append(values)\n",
    "\n",
    "\n",
    "# Hold all final loss values\n",
    "final_loss = []\n",
    "\n",
    "# Iterate through sizes \n",
    "for el in range(len(small_size)):\n",
    "\n",
    "    # Iterate through splits\n",
    "    for el2 in range(len(splits)):\n",
    "\n",
    "        print(\"Element Size Test:\", el, \"and Crossfold Number:\", el2)\n",
    "\n",
    "        # Define groups (thankfully always 2 00-week and 6 post-00 week)\n",
    "        groups = torch.tensor([0,0,1,1,1,1,1,1])\n",
    "\n",
    "        # Define models \n",
    "        rna_16s_train = simple_FC(input_size = rna_16s.shape[0], hidden_sizes = [small_size[el], 64], prediction_dim = 2, dropout = 0.5)\n",
    "        metap_train = simple_FC(input_size = metaproteomics.shape[0], hidden_sizes = [large_size[el], 64], prediction_dim = 2, dropout = 0.5)\n",
    "        metab_pos_train = simple_FC(input_size = metabpos.shape[0], hidden_sizes = [large_size[el], 64], prediction_dim = 2, dropout = 0.5)\n",
    "        metab_neg_train = simple_FC(input_size = metabneg.shape[0], hidden_sizes = [large_size[el], 64], prediction_dim = 2, dropout = 0.5)\n",
    "\n",
    "        # Define joint model\n",
    "        joint_mlp = JointMLP(marginal_models = [rna_16s_train, metap_train, metab_pos_train, metab_neg_train], hidden_dim = 64, hooks=False)\n",
    "\n",
    "        # Optimize parameters \n",
    "        optimizer_mlp = torch.optim.AdamW(joint_mlp.parameters(), lr=1e-4)\n",
    "\n",
    "        # Define data views with subsetting\n",
    "        views = [torch.tensor(rna_16s.iloc[:,splits[el2]].T.values, dtype = torch.float32),\n",
    "                torch.tensor(metaproteomics.iloc[:,splits[el2]].T.values, dtype = torch.float32),\n",
    "                torch.tensor(metabpos.iloc[:,splits[el2]].T.values, dtype = torch.float32),\n",
    "                torch.tensor(metabneg.iloc[:,splits[el2]].T.values, dtype = torch.float32)]\n",
    "\n",
    "        acc_mlp = []\n",
    "\n",
    "        # Time: 20 seconds             \n",
    "        for i in range(1500):\n",
    "\n",
    "            # Update the mlp\n",
    "            yhat, h, yhats, hiddens = joint_mlp(*views)\n",
    "\n",
    "            # pass the predictions and distributions to the loss function and update parameters\n",
    "            _, _, loss = joint_mlp.loss(groups, yhat, yhats)\n",
    "\n",
    "            optimizer_mlp.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(joint_mlp.parameters(), 2.0)\n",
    "            optimizer_mlp.step()\n",
    "\n",
    "        final_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [0.25 for x in range(15)]\n",
    "sizes.extend([0.5 for x in range(15)])\n",
    "sizes.extend([1 for x in range(15)])\n",
    "sizes.extend([2 for x in range(15)])\n",
    "sizes.extend([4 for x in range(15)])\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Size\": sizes,\n",
    "    \"Loss\": final_loss\n",
    "}).to_csv(\"DeepIMV_tuning.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all four models. Predicition dim is the number of categories.\n",
    "rna_16s_model = simple_FC(input_size = rna_16s.shape[0], hidden_sizes = [16, 64], prediction_dim = 2, dropout = 0.5)\n",
    "metap_model = simple_FC(input_size = metaproteomics.shape[0], hidden_sizes = [4096, 64], prediction_dim = 2, dropout = 0.5)\n",
    "metab_pos_model = simple_FC(input_size = metabpos.shape[0], hidden_sizes = [4096, 64], prediction_dim = 2, dropout = 0.5)\n",
    "metab_neg_model = simple_FC(input_size = metabneg.shape[0], hidden_sizes = [4096, 64], prediction_dim = 2, dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define joint model\n",
    "joint_mlp = JointMLP(marginal_models = [rna_16s_model, metap_model, metab_pos_model, metab_neg_model], hidden_dim = 64, hooks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize parameters \n",
    "optimizer_mlp = torch.optim.AdamW(joint_mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.390\n",
      "Epoch 2 loss: 0.342\n",
      "Epoch 3 loss: 0.310\n",
      "Epoch 4 loss: 0.293\n",
      "Epoch 5 loss: 0.258\n",
      "Epoch 6 loss: 0.213\n",
      "Epoch 7 loss: 0.236\n",
      "Epoch 8 loss: 0.206\n",
      "Epoch 9 loss: 0.207\n",
      "Epoch 10 loss: 0.215\n",
      "Epoch 11 loss: 0.169\n",
      "Epoch 12 loss: 0.197\n",
      "Epoch 13 loss: 0.170\n",
      "Epoch 14 loss: 0.148\n",
      "Epoch 15 loss: 0.154\n",
      "Epoch 16 loss: 0.159\n",
      "Epoch 17 loss: 0.122\n",
      "Epoch 18 loss: 0.121\n",
      "Epoch 19 loss: 0.148\n",
      "Epoch 20 loss: 0.120\n",
      "Epoch 21 loss: 0.143\n",
      "Epoch 22 loss: 0.121\n",
      "Epoch 23 loss: 0.135\n",
      "Epoch 24 loss: 0.133\n",
      "Epoch 25 loss: 0.116\n",
      "Epoch 26 loss: 0.099\n",
      "Epoch 27 loss: 0.118\n",
      "Epoch 28 loss: 0.122\n",
      "Epoch 29 loss: 0.109\n",
      "Epoch 30 loss: 0.106\n",
      "Epoch 31 loss: 0.142\n",
      "Epoch 32 loss: 0.100\n",
      "Epoch 33 loss: 0.087\n",
      "Epoch 34 loss: 0.113\n",
      "Epoch 35 loss: 0.096\n",
      "Epoch 36 loss: 0.093\n",
      "Epoch 37 loss: 0.096\n",
      "Epoch 38 loss: 0.104\n",
      "Epoch 39 loss: 0.103\n",
      "Epoch 40 loss: 0.106\n",
      "Epoch 41 loss: 0.095\n",
      "Epoch 42 loss: 0.093\n",
      "Epoch 43 loss: 0.106\n",
      "Epoch 44 loss: 0.093\n",
      "Epoch 45 loss: 0.112\n",
      "Epoch 46 loss: 0.089\n",
      "Epoch 47 loss: 0.104\n",
      "Epoch 48 loss: 0.110\n",
      "Epoch 49 loss: 0.098\n",
      "Epoch 50 loss: 0.098\n",
      "Epoch 51 loss: 0.117\n",
      "Epoch 52 loss: 0.098\n",
      "Epoch 53 loss: 0.113\n",
      "Epoch 54 loss: 0.101\n",
      "Epoch 55 loss: 0.106\n",
      "Epoch 56 loss: 0.129\n",
      "Epoch 57 loss: 0.091\n",
      "Epoch 58 loss: 0.089\n",
      "Epoch 59 loss: 0.111\n",
      "Epoch 60 loss: 0.119\n",
      "Epoch 61 loss: 0.090\n",
      "Epoch 62 loss: 0.098\n",
      "Epoch 63 loss: 0.092\n",
      "Epoch 64 loss: 0.105\n",
      "Epoch 65 loss: 0.108\n",
      "Epoch 66 loss: 0.097\n",
      "Epoch 67 loss: 0.091\n",
      "Epoch 68 loss: 0.101\n",
      "Epoch 69 loss: 0.108\n",
      "Epoch 70 loss: 0.089\n",
      "Epoch 71 loss: 0.081\n",
      "Epoch 72 loss: 0.081\n",
      "Epoch 73 loss: 0.097\n",
      "Epoch 74 loss: 0.083\n",
      "Epoch 75 loss: 0.086\n",
      "Epoch 76 loss: 0.073\n",
      "Epoch 77 loss: 0.086\n",
      "Epoch 78 loss: 0.084\n",
      "Epoch 79 loss: 0.096\n",
      "Epoch 80 loss: 0.084\n",
      "Epoch 81 loss: 0.095\n",
      "Epoch 82 loss: 0.068\n",
      "Epoch 83 loss: 0.081\n",
      "Epoch 84 loss: 0.090\n",
      "Epoch 85 loss: 0.071\n",
      "Epoch 86 loss: 0.115\n",
      "Epoch 87 loss: 0.140\n",
      "Epoch 88 loss: 0.077\n",
      "Epoch 89 loss: 0.078\n",
      "Epoch 90 loss: 0.107\n",
      "Epoch 91 loss: 0.091\n",
      "Epoch 92 loss: 0.076\n",
      "Epoch 93 loss: 0.091\n",
      "Epoch 94 loss: 0.058\n",
      "Epoch 95 loss: 0.074\n",
      "Epoch 96 loss: 0.087\n",
      "Epoch 97 loss: 0.075\n",
      "Epoch 98 loss: 0.073\n",
      "Epoch 99 loss: 0.087\n",
      "Epoch 100 loss: 0.094\n",
      "Epoch 101 loss: 0.083\n",
      "Epoch 102 loss: 0.112\n",
      "Epoch 103 loss: 0.087\n",
      "Epoch 104 loss: 0.080\n",
      "Epoch 105 loss: 0.083\n",
      "Epoch 106 loss: 0.104\n",
      "Epoch 107 loss: 0.099\n",
      "Epoch 108 loss: 0.093\n",
      "Epoch 109 loss: 0.093\n",
      "Epoch 110 loss: 0.082\n",
      "Epoch 111 loss: 0.073\n",
      "Epoch 112 loss: 0.083\n",
      "Epoch 113 loss: 0.089\n",
      "Epoch 114 loss: 0.090\n",
      "Epoch 115 loss: 0.086\n",
      "Epoch 116 loss: 0.091\n",
      "Epoch 117 loss: 0.075\n",
      "Epoch 118 loss: 0.083\n",
      "Epoch 119 loss: 0.097\n",
      "Epoch 120 loss: 0.098\n",
      "Epoch 121 loss: 0.091\n",
      "Epoch 122 loss: 0.076\n",
      "Epoch 123 loss: 0.099\n",
      "Epoch 124 loss: 0.107\n",
      "Epoch 125 loss: 0.095\n",
      "Epoch 126 loss: 0.092\n",
      "Epoch 127 loss: 0.074\n",
      "Epoch 128 loss: 0.086\n",
      "Epoch 129 loss: 0.070\n",
      "Epoch 130 loss: 0.079\n",
      "Epoch 131 loss: 0.107\n",
      "Epoch 132 loss: 0.077\n",
      "Epoch 133 loss: 0.091\n",
      "Epoch 134 loss: 0.078\n",
      "Epoch 135 loss: 0.091\n",
      "Epoch 136 loss: 0.090\n",
      "Epoch 137 loss: 0.081\n",
      "Epoch 138 loss: 0.075\n",
      "Epoch 139 loss: 0.087\n",
      "Epoch 140 loss: 0.077\n",
      "Epoch 141 loss: 0.079\n",
      "Epoch 142 loss: 0.076\n",
      "Epoch 143 loss: 0.086\n",
      "Epoch 144 loss: 0.074\n",
      "Epoch 145 loss: 0.102\n",
      "Epoch 146 loss: 0.058\n",
      "Epoch 147 loss: 0.076\n",
      "Epoch 148 loss: 0.097\n",
      "Epoch 149 loss: 0.060\n",
      "Epoch 150 loss: 0.074\n",
      "Epoch 151 loss: 0.073\n",
      "Epoch 152 loss: 0.073\n",
      "Epoch 153 loss: 0.077\n",
      "Epoch 154 loss: 0.089\n",
      "Epoch 155 loss: 0.085\n",
      "Epoch 156 loss: 0.094\n",
      "Epoch 157 loss: 0.075\n",
      "Epoch 158 loss: 0.087\n",
      "Epoch 159 loss: 0.111\n",
      "Epoch 160 loss: 0.074\n",
      "Epoch 161 loss: 0.081\n",
      "Epoch 162 loss: 0.080\n",
      "Epoch 163 loss: 0.085\n",
      "Epoch 164 loss: 0.091\n",
      "Epoch 165 loss: 0.087\n",
      "Epoch 166 loss: 0.087\n",
      "Epoch 167 loss: 0.084\n",
      "Epoch 168 loss: 0.078\n",
      "Epoch 169 loss: 0.057\n",
      "Epoch 170 loss: 0.079\n",
      "Epoch 171 loss: 0.098\n",
      "Epoch 172 loss: 0.085\n",
      "Epoch 173 loss: 0.077\n",
      "Epoch 174 loss: 0.067\n",
      "Epoch 175 loss: 0.080\n",
      "Epoch 176 loss: 0.058\n",
      "Epoch 177 loss: 0.063\n",
      "Epoch 178 loss: 0.085\n",
      "Epoch 179 loss: 0.072\n",
      "Epoch 180 loss: 0.052\n",
      "Epoch 181 loss: 0.062\n",
      "Epoch 182 loss: 0.094\n",
      "Epoch 183 loss: 0.089\n",
      "Epoch 184 loss: 0.081\n",
      "Epoch 185 loss: 0.125\n",
      "Epoch 186 loss: 0.081\n",
      "Epoch 187 loss: 0.071\n",
      "Epoch 188 loss: 0.070\n",
      "Epoch 189 loss: 0.079\n",
      "Epoch 190 loss: 0.091\n",
      "Epoch 191 loss: 0.095\n",
      "Epoch 192 loss: 0.090\n",
      "Epoch 193 loss: 0.085\n",
      "Epoch 194 loss: 0.078\n",
      "Epoch 195 loss: 0.063\n",
      "Epoch 196 loss: 0.086\n",
      "Epoch 197 loss: 0.080\n",
      "Epoch 198 loss: 0.069\n",
      "Epoch 199 loss: 0.104\n",
      "Epoch 200 loss: 0.075\n",
      "Epoch 201 loss: 0.076\n",
      "Epoch 202 loss: 0.089\n",
      "Epoch 203 loss: 0.084\n",
      "Epoch 204 loss: 0.085\n",
      "Epoch 205 loss: 0.065\n",
      "Epoch 206 loss: 0.090\n",
      "Epoch 207 loss: 0.061\n",
      "Epoch 208 loss: 0.106\n",
      "Epoch 209 loss: 0.083\n",
      "Epoch 210 loss: 0.094\n",
      "Epoch 211 loss: 0.069\n",
      "Epoch 212 loss: 0.084\n",
      "Epoch 213 loss: 0.077\n",
      "Epoch 214 loss: 0.080\n",
      "Epoch 215 loss: 0.090\n",
      "Epoch 216 loss: 0.076\n",
      "Epoch 217 loss: 0.085\n",
      "Epoch 218 loss: 0.077\n",
      "Epoch 219 loss: 0.075\n",
      "Epoch 220 loss: 0.067\n",
      "Epoch 221 loss: 0.087\n",
      "Epoch 222 loss: 0.081\n",
      "Epoch 223 loss: 0.060\n",
      "Epoch 224 loss: 0.073\n",
      "Epoch 225 loss: 0.093\n",
      "Epoch 226 loss: 0.068\n",
      "Epoch 227 loss: 0.062\n",
      "Epoch 228 loss: 0.062\n",
      "Epoch 229 loss: 0.070\n",
      "Epoch 230 loss: 0.056\n",
      "Epoch 231 loss: 0.095\n",
      "Epoch 232 loss: 0.081\n",
      "Epoch 233 loss: 0.073\n",
      "Epoch 234 loss: 0.070\n",
      "Epoch 235 loss: 0.068\n",
      "Epoch 236 loss: 0.070\n",
      "Epoch 237 loss: 0.091\n",
      "Epoch 238 loss: 0.058\n",
      "Epoch 239 loss: 0.078\n",
      "Epoch 240 loss: 0.101\n",
      "Epoch 241 loss: 0.079\n",
      "Epoch 242 loss: 0.089\n",
      "Epoch 243 loss: 0.050\n",
      "Epoch 244 loss: 0.068\n",
      "Epoch 245 loss: 0.073\n",
      "Epoch 246 loss: 0.082\n",
      "Epoch 247 loss: 0.081\n",
      "Epoch 248 loss: 0.085\n",
      "Epoch 249 loss: 0.068\n",
      "Epoch 250 loss: 0.070\n",
      "Epoch 251 loss: 0.086\n",
      "Epoch 252 loss: 0.057\n",
      "Epoch 253 loss: 0.087\n",
      "Epoch 254 loss: 0.059\n",
      "Epoch 255 loss: 0.060\n",
      "Epoch 256 loss: 0.080\n",
      "Epoch 257 loss: 0.081\n",
      "Epoch 258 loss: 0.088\n",
      "Epoch 259 loss: 0.073\n",
      "Epoch 260 loss: 0.063\n",
      "Epoch 261 loss: 0.087\n",
      "Epoch 262 loss: 0.080\n",
      "Epoch 263 loss: 0.055\n",
      "Epoch 264 loss: 0.082\n",
      "Epoch 265 loss: 0.087\n",
      "Epoch 266 loss: 0.100\n",
      "Epoch 267 loss: 0.085\n",
      "Epoch 268 loss: 0.107\n",
      "Epoch 269 loss: 0.054\n",
      "Epoch 270 loss: 0.074\n",
      "Epoch 271 loss: 0.065\n",
      "Epoch 272 loss: 0.078\n",
      "Epoch 273 loss: 0.051\n",
      "Epoch 274 loss: 0.067\n",
      "Epoch 275 loss: 0.085\n",
      "Epoch 276 loss: 0.065\n",
      "Epoch 277 loss: 0.071\n",
      "Epoch 278 loss: 0.072\n",
      "Epoch 279 loss: 0.075\n",
      "Epoch 280 loss: 0.084\n",
      "Epoch 281 loss: 0.080\n",
      "Epoch 282 loss: 0.064\n",
      "Epoch 283 loss: 0.055\n",
      "Epoch 284 loss: 0.070\n",
      "Epoch 285 loss: 0.079\n",
      "Epoch 286 loss: 0.076\n",
      "Epoch 287 loss: 0.079\n",
      "Epoch 288 loss: 0.076\n",
      "Epoch 289 loss: 0.079\n",
      "Epoch 290 loss: 0.062\n",
      "Epoch 291 loss: 0.072\n",
      "Epoch 292 loss: 0.068\n",
      "Epoch 293 loss: 0.076\n",
      "Epoch 294 loss: 0.078\n",
      "Epoch 295 loss: 0.079\n",
      "Epoch 296 loss: 0.076\n",
      "Epoch 297 loss: 0.074\n",
      "Epoch 298 loss: 0.071\n",
      "Epoch 299 loss: 0.087\n",
      "Epoch 300 loss: 0.079\n",
      "Epoch 301 loss: 0.086\n",
      "Epoch 302 loss: 0.075\n",
      "Epoch 303 loss: 0.093\n",
      "Epoch 304 loss: 0.061\n",
      "Epoch 305 loss: 0.072\n",
      "Epoch 306 loss: 0.070\n",
      "Epoch 307 loss: 0.072\n",
      "Epoch 308 loss: 0.073\n",
      "Epoch 309 loss: 0.074\n",
      "Epoch 310 loss: 0.062\n",
      "Epoch 311 loss: 0.077\n",
      "Epoch 312 loss: 0.059\n",
      "Epoch 313 loss: 0.053\n",
      "Epoch 314 loss: 0.070\n",
      "Epoch 315 loss: 0.067\n",
      "Epoch 316 loss: 0.076\n",
      "Epoch 317 loss: 0.048\n",
      "Epoch 318 loss: 0.088\n",
      "Epoch 319 loss: 0.092\n",
      "Epoch 320 loss: 0.066\n",
      "Epoch 321 loss: 0.069\n",
      "Epoch 322 loss: 0.079\n",
      "Epoch 323 loss: 0.079\n",
      "Epoch 324 loss: 0.057\n",
      "Epoch 325 loss: 0.076\n",
      "Epoch 326 loss: 0.059\n",
      "Epoch 327 loss: 0.099\n",
      "Epoch 328 loss: 0.068\n",
      "Epoch 329 loss: 0.060\n",
      "Epoch 330 loss: 0.068\n",
      "Epoch 331 loss: 0.081\n",
      "Epoch 332 loss: 0.057\n",
      "Epoch 333 loss: 0.091\n",
      "Epoch 334 loss: 0.062\n",
      "Epoch 335 loss: 0.081\n",
      "Epoch 336 loss: 0.066\n",
      "Epoch 337 loss: 0.082\n",
      "Epoch 338 loss: 0.073\n",
      "Epoch 339 loss: 0.075\n",
      "Epoch 340 loss: 0.059\n",
      "Epoch 341 loss: 0.091\n",
      "Epoch 342 loss: 0.068\n",
      "Epoch 343 loss: 0.068\n",
      "Epoch 344 loss: 0.079\n",
      "Epoch 345 loss: 0.078\n",
      "Epoch 346 loss: 0.090\n",
      "Epoch 347 loss: 0.069\n",
      "Epoch 348 loss: 0.068\n",
      "Epoch 349 loss: 0.084\n",
      "Epoch 350 loss: 0.061\n",
      "Epoch 351 loss: 0.073\n",
      "Epoch 352 loss: 0.074\n",
      "Epoch 353 loss: 0.070\n",
      "Epoch 354 loss: 0.084\n",
      "Epoch 355 loss: 0.077\n",
      "Epoch 356 loss: 0.085\n",
      "Epoch 357 loss: 0.070\n",
      "Epoch 358 loss: 0.090\n",
      "Epoch 359 loss: 0.074\n",
      "Epoch 360 loss: 0.088\n",
      "Epoch 361 loss: 0.074\n",
      "Epoch 362 loss: 0.079\n",
      "Epoch 363 loss: 0.080\n",
      "Epoch 364 loss: 0.051\n",
      "Epoch 365 loss: 0.072\n",
      "Epoch 366 loss: 0.054\n",
      "Epoch 367 loss: 0.067\n",
      "Epoch 368 loss: 0.071\n",
      "Epoch 369 loss: 0.056\n",
      "Epoch 370 loss: 0.059\n",
      "Epoch 371 loss: 0.058\n",
      "Epoch 372 loss: 0.097\n",
      "Epoch 373 loss: 0.061\n",
      "Epoch 374 loss: 0.070\n",
      "Epoch 375 loss: 0.069\n",
      "Epoch 376 loss: 0.075\n",
      "Epoch 377 loss: 0.086\n",
      "Epoch 378 loss: 0.071\n",
      "Epoch 379 loss: 0.066\n",
      "Epoch 380 loss: 0.062\n",
      "Epoch 381 loss: 0.055\n",
      "Epoch 382 loss: 0.078\n",
      "Epoch 383 loss: 0.056\n",
      "Epoch 384 loss: 0.088\n",
      "Epoch 385 loss: 0.063\n",
      "Epoch 386 loss: 0.078\n",
      "Epoch 387 loss: 0.077\n",
      "Epoch 388 loss: 0.062\n",
      "Epoch 389 loss: 0.070\n",
      "Epoch 390 loss: 0.074\n",
      "Epoch 391 loss: 0.094\n",
      "Epoch 392 loss: 0.076\n",
      "Epoch 393 loss: 0.094\n",
      "Epoch 394 loss: 0.076\n",
      "Epoch 395 loss: 0.092\n",
      "Epoch 396 loss: 0.076\n",
      "Epoch 397 loss: 0.056\n",
      "Epoch 398 loss: 0.071\n",
      "Epoch 399 loss: 0.070\n",
      "Epoch 400 loss: 0.074\n",
      "Epoch 401 loss: 0.096\n",
      "Epoch 402 loss: 0.088\n",
      "Epoch 403 loss: 0.052\n",
      "Epoch 404 loss: 0.069\n",
      "Epoch 405 loss: 0.076\n",
      "Epoch 406 loss: 0.061\n",
      "Epoch 407 loss: 0.070\n",
      "Epoch 408 loss: 0.060\n",
      "Epoch 409 loss: 0.067\n",
      "Epoch 410 loss: 0.073\n",
      "Epoch 411 loss: 0.068\n",
      "Epoch 412 loss: 0.091\n",
      "Epoch 413 loss: 0.048\n",
      "Epoch 414 loss: 0.066\n",
      "Epoch 415 loss: 0.075\n",
      "Epoch 416 loss: 0.069\n",
      "Epoch 417 loss: 0.093\n",
      "Epoch 418 loss: 0.081\n",
      "Epoch 419 loss: 0.047\n",
      "Epoch 420 loss: 0.056\n",
      "Epoch 421 loss: 0.073\n",
      "Epoch 422 loss: 0.072\n",
      "Epoch 423 loss: 0.063\n",
      "Epoch 424 loss: 0.062\n",
      "Epoch 425 loss: 0.071\n",
      "Epoch 426 loss: 0.059\n",
      "Epoch 427 loss: 0.059\n",
      "Epoch 428 loss: 0.075\n",
      "Epoch 429 loss: 0.069\n",
      "Epoch 430 loss: 0.077\n",
      "Epoch 431 loss: 0.058\n",
      "Epoch 432 loss: 0.068\n",
      "Epoch 433 loss: 0.053\n",
      "Epoch 434 loss: 0.086\n",
      "Epoch 435 loss: 0.066\n",
      "Epoch 436 loss: 0.063\n",
      "Epoch 437 loss: 0.049\n",
      "Epoch 438 loss: 0.066\n",
      "Epoch 439 loss: 0.073\n",
      "Epoch 440 loss: 0.080\n",
      "Epoch 441 loss: 0.063\n",
      "Epoch 442 loss: 0.058\n",
      "Epoch 443 loss: 0.058\n",
      "Epoch 444 loss: 0.073\n",
      "Epoch 445 loss: 0.062\n",
      "Epoch 446 loss: 0.070\n",
      "Epoch 447 loss: 0.077\n",
      "Epoch 448 loss: 0.057\n",
      "Epoch 449 loss: 0.075\n",
      "Epoch 450 loss: 0.076\n",
      "Epoch 451 loss: 0.069\n",
      "Epoch 452 loss: 0.070\n",
      "Epoch 453 loss: 0.058\n",
      "Epoch 454 loss: 0.065\n",
      "Epoch 455 loss: 0.064\n",
      "Epoch 456 loss: 0.060\n",
      "Epoch 457 loss: 0.061\n",
      "Epoch 458 loss: 0.052\n",
      "Epoch 459 loss: 0.081\n",
      "Epoch 460 loss: 0.090\n",
      "Epoch 461 loss: 0.080\n",
      "Epoch 462 loss: 0.051\n",
      "Epoch 463 loss: 0.073\n",
      "Epoch 464 loss: 0.070\n",
      "Epoch 465 loss: 0.066\n",
      "Epoch 466 loss: 0.049\n",
      "Epoch 467 loss: 0.059\n",
      "Epoch 468 loss: 0.051\n",
      "Epoch 469 loss: 0.059\n",
      "Epoch 470 loss: 0.063\n",
      "Epoch 471 loss: 0.070\n",
      "Epoch 472 loss: 0.066\n",
      "Epoch 473 loss: 0.069\n",
      "Epoch 474 loss: 0.085\n",
      "Epoch 475 loss: 0.063\n",
      "Epoch 476 loss: 0.082\n",
      "Epoch 477 loss: 0.072\n",
      "Epoch 478 loss: 0.052\n",
      "Epoch 479 loss: 0.049\n",
      "Epoch 480 loss: 0.089\n",
      "Epoch 481 loss: 0.082\n",
      "Epoch 482 loss: 0.048\n",
      "Epoch 483 loss: 0.073\n",
      "Epoch 484 loss: 0.052\n",
      "Epoch 485 loss: 0.048\n",
      "Epoch 486 loss: 0.064\n",
      "Epoch 487 loss: 0.072\n",
      "Epoch 488 loss: 0.044\n",
      "Epoch 489 loss: 0.070\n",
      "Epoch 490 loss: 0.061\n",
      "Epoch 491 loss: 0.085\n",
      "Epoch 492 loss: 0.070\n",
      "Epoch 493 loss: 0.065\n",
      "Epoch 494 loss: 0.046\n",
      "Epoch 495 loss: 0.074\n",
      "Epoch 496 loss: 0.076\n",
      "Epoch 497 loss: 0.079\n",
      "Epoch 498 loss: 0.066\n",
      "Epoch 499 loss: 0.074\n",
      "Epoch 500 loss: 0.065\n",
      "Epoch 501 loss: 0.066\n",
      "Epoch 502 loss: 0.048\n",
      "Epoch 503 loss: 0.060\n",
      "Epoch 504 loss: 0.076\n",
      "Epoch 505 loss: 0.066\n",
      "Epoch 506 loss: 0.073\n",
      "Epoch 507 loss: 0.068\n",
      "Epoch 508 loss: 0.072\n",
      "Epoch 509 loss: 0.043\n",
      "Epoch 510 loss: 0.059\n",
      "Epoch 511 loss: 0.043\n",
      "Epoch 512 loss: 0.072\n",
      "Epoch 513 loss: 0.076\n",
      "Epoch 514 loss: 0.068\n",
      "Epoch 515 loss: 0.066\n",
      "Epoch 516 loss: 0.055\n",
      "Epoch 517 loss: 0.060\n",
      "Epoch 518 loss: 0.043\n",
      "Epoch 519 loss: 0.063\n",
      "Epoch 520 loss: 0.075\n",
      "Epoch 521 loss: 0.074\n",
      "Epoch 522 loss: 0.061\n",
      "Epoch 523 loss: 0.056\n",
      "Epoch 524 loss: 0.061\n",
      "Epoch 525 loss: 0.071\n",
      "Epoch 526 loss: 0.069\n",
      "Epoch 527 loss: 0.080\n",
      "Epoch 528 loss: 0.052\n",
      "Epoch 529 loss: 0.065\n",
      "Epoch 530 loss: 0.066\n",
      "Epoch 531 loss: 0.057\n",
      "Epoch 532 loss: 0.073\n",
      "Epoch 533 loss: 0.074\n",
      "Epoch 534 loss: 0.053\n",
      "Epoch 535 loss: 0.070\n",
      "Epoch 536 loss: 0.061\n",
      "Epoch 537 loss: 0.072\n",
      "Epoch 538 loss: 0.072\n",
      "Epoch 539 loss: 0.040\n",
      "Epoch 540 loss: 0.070\n",
      "Epoch 541 loss: 0.057\n",
      "Epoch 542 loss: 0.077\n",
      "Epoch 543 loss: 0.060\n",
      "Epoch 544 loss: 0.090\n",
      "Epoch 545 loss: 0.078\n",
      "Epoch 546 loss: 0.063\n",
      "Epoch 547 loss: 0.058\n",
      "Epoch 548 loss: 0.079\n",
      "Epoch 549 loss: 0.068\n",
      "Epoch 550 loss: 0.075\n",
      "Epoch 551 loss: 0.064\n",
      "Epoch 552 loss: 0.083\n",
      "Epoch 553 loss: 0.067\n",
      "Epoch 554 loss: 0.069\n",
      "Epoch 555 loss: 0.059\n",
      "Epoch 556 loss: 0.066\n",
      "Epoch 557 loss: 0.056\n",
      "Epoch 558 loss: 0.068\n",
      "Epoch 559 loss: 0.050\n",
      "Epoch 560 loss: 0.058\n",
      "Epoch 561 loss: 0.052\n",
      "Epoch 562 loss: 0.051\n",
      "Epoch 563 loss: 0.067\n",
      "Epoch 564 loss: 0.063\n",
      "Epoch 565 loss: 0.064\n",
      "Epoch 566 loss: 0.081\n",
      "Epoch 567 loss: 0.083\n",
      "Epoch 568 loss: 0.065\n",
      "Epoch 569 loss: 0.045\n",
      "Epoch 570 loss: 0.047\n",
      "Epoch 571 loss: 0.063\n",
      "Epoch 572 loss: 0.064\n",
      "Epoch 573 loss: 0.068\n",
      "Epoch 574 loss: 0.088\n",
      "Epoch 575 loss: 0.052\n",
      "Epoch 576 loss: 0.066\n",
      "Epoch 577 loss: 0.046\n",
      "Epoch 578 loss: 0.071\n",
      "Epoch 579 loss: 0.069\n",
      "Epoch 580 loss: 0.055\n",
      "Epoch 581 loss: 0.056\n",
      "Epoch 582 loss: 0.063\n",
      "Epoch 583 loss: 0.078\n",
      "Epoch 584 loss: 0.066\n",
      "Epoch 585 loss: 0.053\n",
      "Epoch 586 loss: 0.063\n",
      "Epoch 587 loss: 0.060\n",
      "Epoch 588 loss: 0.063\n",
      "Epoch 589 loss: 0.071\n",
      "Epoch 590 loss: 0.070\n",
      "Epoch 591 loss: 0.057\n",
      "Epoch 592 loss: 0.065\n",
      "Epoch 593 loss: 0.079\n",
      "Epoch 594 loss: 0.063\n",
      "Epoch 595 loss: 0.082\n",
      "Epoch 596 loss: 0.055\n",
      "Epoch 597 loss: 0.060\n",
      "Epoch 598 loss: 0.067\n",
      "Epoch 599 loss: 0.069\n",
      "Epoch 600 loss: 0.068\n",
      "Epoch 601 loss: 0.062\n",
      "Epoch 602 loss: 0.074\n",
      "Epoch 603 loss: 0.088\n",
      "Epoch 604 loss: 0.058\n",
      "Epoch 605 loss: 0.051\n",
      "Epoch 606 loss: 0.068\n",
      "Epoch 607 loss: 0.056\n",
      "Epoch 608 loss: 0.060\n",
      "Epoch 609 loss: 0.065\n",
      "Epoch 610 loss: 0.057\n",
      "Epoch 611 loss: 0.047\n",
      "Epoch 612 loss: 0.062\n",
      "Epoch 613 loss: 0.063\n",
      "Epoch 614 loss: 0.063\n",
      "Epoch 615 loss: 0.056\n",
      "Epoch 616 loss: 0.086\n",
      "Epoch 617 loss: 0.067\n",
      "Epoch 618 loss: 0.060\n",
      "Epoch 619 loss: 0.065\n",
      "Epoch 620 loss: 0.052\n",
      "Epoch 621 loss: 0.087\n",
      "Epoch 622 loss: 0.061\n",
      "Epoch 623 loss: 0.061\n",
      "Epoch 624 loss: 0.047\n",
      "Epoch 625 loss: 0.055\n",
      "Epoch 626 loss: 0.041\n",
      "Epoch 627 loss: 0.054\n",
      "Epoch 628 loss: 0.045\n",
      "Epoch 629 loss: 0.060\n",
      "Epoch 630 loss: 0.077\n",
      "Epoch 631 loss: 0.073\n",
      "Epoch 632 loss: 0.054\n",
      "Epoch 633 loss: 0.043\n",
      "Epoch 634 loss: 0.056\n",
      "Epoch 635 loss: 0.078\n",
      "Epoch 636 loss: 0.059\n",
      "Epoch 637 loss: 0.076\n",
      "Epoch 638 loss: 0.047\n",
      "Epoch 639 loss: 0.068\n",
      "Epoch 640 loss: 0.055\n",
      "Epoch 641 loss: 0.069\n",
      "Epoch 642 loss: 0.073\n",
      "Epoch 643 loss: 0.059\n",
      "Epoch 644 loss: 0.069\n",
      "Epoch 645 loss: 0.091\n",
      "Epoch 646 loss: 0.049\n",
      "Epoch 647 loss: 0.063\n",
      "Epoch 648 loss: 0.058\n",
      "Epoch 649 loss: 0.062\n",
      "Epoch 650 loss: 0.057\n",
      "Epoch 651 loss: 0.067\n",
      "Epoch 652 loss: 0.075\n",
      "Epoch 653 loss: 0.062\n",
      "Epoch 654 loss: 0.054\n",
      "Epoch 655 loss: 0.070\n",
      "Epoch 656 loss: 0.058\n",
      "Epoch 657 loss: 0.069\n",
      "Epoch 658 loss: 0.062\n",
      "Epoch 659 loss: 0.045\n",
      "Epoch 660 loss: 0.074\n",
      "Epoch 661 loss: 0.062\n",
      "Epoch 662 loss: 0.055\n",
      "Epoch 663 loss: 0.054\n",
      "Epoch 664 loss: 0.062\n",
      "Epoch 665 loss: 0.056\n",
      "Epoch 666 loss: 0.079\n",
      "Epoch 667 loss: 0.072\n",
      "Epoch 668 loss: 0.059\n",
      "Epoch 669 loss: 0.044\n",
      "Epoch 670 loss: 0.080\n",
      "Epoch 671 loss: 0.052\n",
      "Epoch 672 loss: 0.054\n",
      "Epoch 673 loss: 0.045\n",
      "Epoch 674 loss: 0.071\n",
      "Epoch 675 loss: 0.074\n",
      "Epoch 676 loss: 0.073\n",
      "Epoch 677 loss: 0.054\n",
      "Epoch 678 loss: 0.055\n",
      "Epoch 679 loss: 0.071\n",
      "Epoch 680 loss: 0.058\n",
      "Epoch 681 loss: 0.063\n",
      "Epoch 682 loss: 0.061\n",
      "Epoch 683 loss: 0.056\n",
      "Epoch 684 loss: 0.042\n",
      "Epoch 685 loss: 0.058\n",
      "Epoch 686 loss: 0.065\n",
      "Epoch 687 loss: 0.064\n",
      "Epoch 688 loss: 0.063\n",
      "Epoch 689 loss: 0.046\n",
      "Epoch 690 loss: 0.047\n",
      "Epoch 691 loss: 0.071\n",
      "Epoch 692 loss: 0.068\n",
      "Epoch 693 loss: 0.063\n",
      "Epoch 694 loss: 0.066\n",
      "Epoch 695 loss: 0.066\n",
      "Epoch 696 loss: 0.048\n",
      "Epoch 697 loss: 0.052\n",
      "Epoch 698 loss: 0.037\n",
      "Epoch 699 loss: 0.066\n",
      "Epoch 700 loss: 0.060\n",
      "Epoch 701 loss: 0.059\n",
      "Epoch 702 loss: 0.074\n",
      "Epoch 703 loss: 0.062\n",
      "Epoch 704 loss: 0.100\n",
      "Epoch 705 loss: 0.048\n",
      "Epoch 706 loss: 0.067\n",
      "Epoch 707 loss: 0.072\n",
      "Epoch 708 loss: 0.077\n",
      "Epoch 709 loss: 0.068\n",
      "Epoch 710 loss: 0.065\n",
      "Epoch 711 loss: 0.083\n",
      "Epoch 712 loss: 0.081\n",
      "Epoch 713 loss: 0.063\n",
      "Epoch 714 loss: 0.039\n",
      "Epoch 715 loss: 0.061\n",
      "Epoch 716 loss: 0.072\n",
      "Epoch 717 loss: 0.050\n",
      "Epoch 718 loss: 0.055\n",
      "Epoch 719 loss: 0.068\n",
      "Epoch 720 loss: 0.048\n",
      "Epoch 721 loss: 0.059\n",
      "Epoch 722 loss: 0.075\n",
      "Epoch 723 loss: 0.053\n",
      "Epoch 724 loss: 0.081\n",
      "Epoch 725 loss: 0.071\n",
      "Epoch 726 loss: 0.066\n",
      "Epoch 727 loss: 0.053\n",
      "Epoch 728 loss: 0.047\n",
      "Epoch 729 loss: 0.059\n",
      "Epoch 730 loss: 0.038\n",
      "Epoch 731 loss: 0.029\n",
      "Epoch 732 loss: 0.042\n",
      "Epoch 733 loss: 0.063\n",
      "Epoch 734 loss: 0.052\n",
      "Epoch 735 loss: 0.063\n",
      "Epoch 736 loss: 0.054\n",
      "Epoch 737 loss: 0.042\n",
      "Epoch 738 loss: 0.050\n",
      "Epoch 739 loss: 0.060\n",
      "Epoch 740 loss: 0.073\n",
      "Epoch 741 loss: 0.055\n",
      "Epoch 742 loss: 0.056\n",
      "Epoch 743 loss: 0.053\n",
      "Epoch 744 loss: 0.069\n",
      "Epoch 745 loss: 0.060\n",
      "Epoch 746 loss: 0.064\n",
      "Epoch 747 loss: 0.074\n",
      "Epoch 748 loss: 0.060\n",
      "Epoch 749 loss: 0.062\n",
      "Epoch 750 loss: 0.061\n",
      "Epoch 751 loss: 0.041\n",
      "Epoch 752 loss: 0.046\n",
      "Epoch 753 loss: 0.055\n",
      "Epoch 754 loss: 0.051\n",
      "Epoch 755 loss: 0.073\n",
      "Epoch 756 loss: 0.068\n",
      "Epoch 757 loss: 0.066\n",
      "Epoch 758 loss: 0.051\n",
      "Epoch 759 loss: 0.056\n",
      "Epoch 760 loss: 0.056\n",
      "Epoch 761 loss: 0.048\n",
      "Epoch 762 loss: 0.062\n",
      "Epoch 763 loss: 0.052\n",
      "Epoch 764 loss: 0.058\n",
      "Epoch 765 loss: 0.063\n",
      "Epoch 766 loss: 0.050\n",
      "Epoch 767 loss: 0.046\n",
      "Epoch 768 loss: 0.062\n",
      "Epoch 769 loss: 0.045\n",
      "Epoch 770 loss: 0.042\n",
      "Epoch 771 loss: 0.056\n",
      "Epoch 772 loss: 0.034\n",
      "Epoch 773 loss: 0.064\n",
      "Epoch 774 loss: 0.075\n",
      "Epoch 775 loss: 0.060\n",
      "Epoch 776 loss: 0.065\n",
      "Epoch 777 loss: 0.057\n",
      "Epoch 778 loss: 0.071\n",
      "Epoch 779 loss: 0.064\n",
      "Epoch 780 loss: 0.048\n",
      "Epoch 781 loss: 0.066\n",
      "Epoch 782 loss: 0.053\n",
      "Epoch 783 loss: 0.038\n",
      "Epoch 784 loss: 0.066\n",
      "Epoch 785 loss: 0.052\n",
      "Epoch 786 loss: 0.089\n",
      "Epoch 787 loss: 0.046\n",
      "Epoch 788 loss: 0.053\n",
      "Epoch 789 loss: 0.072\n",
      "Epoch 790 loss: 0.030\n",
      "Epoch 791 loss: 0.050\n",
      "Epoch 792 loss: 0.056\n",
      "Epoch 793 loss: 0.063\n",
      "Epoch 794 loss: 0.067\n",
      "Epoch 795 loss: 0.061\n",
      "Epoch 796 loss: 0.066\n",
      "Epoch 797 loss: 0.068\n",
      "Epoch 798 loss: 0.053\n",
      "Epoch 799 loss: 0.057\n",
      "Epoch 800 loss: 0.058\n",
      "Epoch 801 loss: 0.056\n",
      "Epoch 802 loss: 0.073\n",
      "Epoch 803 loss: 0.064\n",
      "Epoch 804 loss: 0.052\n",
      "Epoch 805 loss: 0.050\n",
      "Epoch 806 loss: 0.062\n",
      "Epoch 807 loss: 0.074\n",
      "Epoch 808 loss: 0.073\n",
      "Epoch 809 loss: 0.060\n",
      "Epoch 810 loss: 0.067\n",
      "Epoch 811 loss: 0.051\n",
      "Epoch 812 loss: 0.070\n",
      "Epoch 813 loss: 0.064\n",
      "Epoch 814 loss: 0.076\n",
      "Epoch 815 loss: 0.037\n",
      "Epoch 816 loss: 0.067\n",
      "Epoch 817 loss: 0.075\n",
      "Epoch 818 loss: 0.059\n",
      "Epoch 819 loss: 0.042\n",
      "Epoch 820 loss: 0.074\n",
      "Epoch 821 loss: 0.052\n",
      "Epoch 822 loss: 0.066\n",
      "Epoch 823 loss: 0.045\n",
      "Epoch 824 loss: 0.056\n",
      "Epoch 825 loss: 0.037\n",
      "Epoch 826 loss: 0.065\n",
      "Epoch 827 loss: 0.042\n",
      "Epoch 828 loss: 0.067\n",
      "Epoch 829 loss: 0.047\n",
      "Epoch 830 loss: 0.064\n",
      "Epoch 831 loss: 0.053\n",
      "Epoch 832 loss: 0.060\n",
      "Epoch 833 loss: 0.054\n",
      "Epoch 834 loss: 0.057\n",
      "Epoch 835 loss: 0.058\n",
      "Epoch 836 loss: 0.059\n",
      "Epoch 837 loss: 0.048\n",
      "Epoch 838 loss: 0.052\n",
      "Epoch 839 loss: 0.056\n",
      "Epoch 840 loss: 0.069\n",
      "Epoch 841 loss: 0.062\n",
      "Epoch 842 loss: 0.081\n",
      "Epoch 843 loss: 0.083\n",
      "Epoch 844 loss: 0.044\n",
      "Epoch 845 loss: 0.060\n",
      "Epoch 846 loss: 0.061\n",
      "Epoch 847 loss: 0.083\n",
      "Epoch 848 loss: 0.055\n",
      "Epoch 849 loss: 0.062\n",
      "Epoch 850 loss: 0.061\n",
      "Epoch 851 loss: 0.056\n",
      "Epoch 852 loss: 0.066\n",
      "Epoch 853 loss: 0.063\n",
      "Epoch 854 loss: 0.049\n",
      "Epoch 855 loss: 0.053\n",
      "Epoch 856 loss: 0.066\n",
      "Epoch 857 loss: 0.052\n",
      "Epoch 858 loss: 0.060\n",
      "Epoch 859 loss: 0.061\n",
      "Epoch 860 loss: 0.065\n",
      "Epoch 861 loss: 0.081\n",
      "Epoch 862 loss: 0.063\n",
      "Epoch 863 loss: 0.057\n",
      "Epoch 864 loss: 0.068\n",
      "Epoch 865 loss: 0.056\n",
      "Epoch 866 loss: 0.069\n",
      "Epoch 867 loss: 0.069\n",
      "Epoch 868 loss: 0.054\n",
      "Epoch 869 loss: 0.064\n",
      "Epoch 870 loss: 0.052\n",
      "Epoch 871 loss: 0.060\n",
      "Epoch 872 loss: 0.058\n",
      "Epoch 873 loss: 0.058\n",
      "Epoch 874 loss: 0.066\n",
      "Epoch 875 loss: 0.069\n",
      "Epoch 876 loss: 0.046\n",
      "Epoch 877 loss: 0.070\n",
      "Epoch 878 loss: 0.065\n",
      "Epoch 879 loss: 0.041\n",
      "Epoch 880 loss: 0.040\n",
      "Epoch 881 loss: 0.050\n",
      "Epoch 882 loss: 0.064\n",
      "Epoch 883 loss: 0.061\n",
      "Epoch 884 loss: 0.074\n",
      "Epoch 885 loss: 0.046\n",
      "Epoch 886 loss: 0.062\n",
      "Epoch 887 loss: 0.039\n",
      "Epoch 888 loss: 0.053\n",
      "Epoch 889 loss: 0.040\n",
      "Epoch 890 loss: 0.052\n",
      "Epoch 891 loss: 0.051\n",
      "Epoch 892 loss: 0.066\n",
      "Epoch 893 loss: 0.053\n",
      "Epoch 894 loss: 0.047\n",
      "Epoch 895 loss: 0.045\n",
      "Epoch 896 loss: 0.071\n",
      "Epoch 897 loss: 0.043\n",
      "Epoch 898 loss: 0.058\n",
      "Epoch 899 loss: 0.035\n",
      "Epoch 900 loss: 0.045\n",
      "Epoch 901 loss: 0.058\n",
      "Epoch 902 loss: 0.048\n",
      "Epoch 903 loss: 0.057\n",
      "Epoch 904 loss: 0.062\n",
      "Epoch 905 loss: 0.065\n",
      "Epoch 906 loss: 0.078\n",
      "Epoch 907 loss: 0.064\n",
      "Epoch 908 loss: 0.050\n",
      "Epoch 909 loss: 0.056\n",
      "Epoch 910 loss: 0.053\n",
      "Epoch 911 loss: 0.055\n",
      "Epoch 912 loss: 0.057\n",
      "Epoch 913 loss: 0.049\n",
      "Epoch 914 loss: 0.041\n",
      "Epoch 915 loss: 0.052\n",
      "Epoch 916 loss: 0.041\n",
      "Epoch 917 loss: 0.067\n",
      "Epoch 918 loss: 0.053\n",
      "Epoch 919 loss: 0.064\n",
      "Epoch 920 loss: 0.055\n",
      "Epoch 921 loss: 0.055\n",
      "Epoch 922 loss: 0.084\n",
      "Epoch 923 loss: 0.068\n",
      "Epoch 924 loss: 0.057\n",
      "Epoch 925 loss: 0.073\n",
      "Epoch 926 loss: 0.070\n",
      "Epoch 927 loss: 0.053\n",
      "Epoch 928 loss: 0.054\n",
      "Epoch 929 loss: 0.068\n",
      "Epoch 930 loss: 0.058\n",
      "Epoch 931 loss: 0.067\n",
      "Epoch 932 loss: 0.052\n",
      "Epoch 933 loss: 0.042\n",
      "Epoch 934 loss: 0.050\n",
      "Epoch 935 loss: 0.042\n",
      "Epoch 936 loss: 0.072\n",
      "Epoch 937 loss: 0.048\n",
      "Epoch 938 loss: 0.064\n",
      "Epoch 939 loss: 0.062\n",
      "Epoch 940 loss: 0.068\n",
      "Epoch 941 loss: 0.034\n",
      "Epoch 942 loss: 0.062\n",
      "Epoch 943 loss: 0.068\n",
      "Epoch 944 loss: 0.059\n",
      "Epoch 945 loss: 0.045\n",
      "Epoch 946 loss: 0.051\n",
      "Epoch 947 loss: 0.047\n",
      "Epoch 948 loss: 0.055\n",
      "Epoch 949 loss: 0.083\n",
      "Epoch 950 loss: 0.042\n",
      "Epoch 951 loss: 0.054\n",
      "Epoch 952 loss: 0.065\n",
      "Epoch 953 loss: 0.057\n",
      "Epoch 954 loss: 0.073\n",
      "Epoch 955 loss: 0.054\n",
      "Epoch 956 loss: 0.066\n",
      "Epoch 957 loss: 0.052\n",
      "Epoch 958 loss: 0.052\n",
      "Epoch 959 loss: 0.040\n",
      "Epoch 960 loss: 0.062\n",
      "Epoch 961 loss: 0.062\n",
      "Epoch 962 loss: 0.047\n",
      "Epoch 963 loss: 0.060\n",
      "Epoch 964 loss: 0.038\n",
      "Epoch 965 loss: 0.042\n",
      "Epoch 966 loss: 0.075\n",
      "Epoch 967 loss: 0.062\n",
      "Epoch 968 loss: 0.050\n",
      "Epoch 969 loss: 0.060\n",
      "Epoch 970 loss: 0.055\n",
      "Epoch 971 loss: 0.053\n",
      "Epoch 972 loss: 0.043\n",
      "Epoch 973 loss: 0.051\n",
      "Epoch 974 loss: 0.045\n",
      "Epoch 975 loss: 0.038\n",
      "Epoch 976 loss: 0.047\n",
      "Epoch 977 loss: 0.045\n",
      "Epoch 978 loss: 0.059\n",
      "Epoch 979 loss: 0.038\n",
      "Epoch 980 loss: 0.041\n",
      "Epoch 981 loss: 0.058\n",
      "Epoch 982 loss: 0.044\n",
      "Epoch 983 loss: 0.040\n",
      "Epoch 984 loss: 0.076\n",
      "Epoch 985 loss: 0.076\n",
      "Epoch 986 loss: 0.045\n",
      "Epoch 987 loss: 0.052\n",
      "Epoch 988 loss: 0.051\n",
      "Epoch 989 loss: 0.073\n",
      "Epoch 990 loss: 0.063\n",
      "Epoch 991 loss: 0.040\n",
      "Epoch 992 loss: 0.077\n",
      "Epoch 993 loss: 0.047\n",
      "Epoch 994 loss: 0.057\n",
      "Epoch 995 loss: 0.067\n",
      "Epoch 996 loss: 0.050\n",
      "Epoch 997 loss: 0.048\n",
      "Epoch 998 loss: 0.050\n",
      "Epoch 999 loss: 0.066\n",
      "Epoch 1000 loss: 0.064\n",
      "Epoch 1001 loss: 0.039\n",
      "Epoch 1002 loss: 0.047\n",
      "Epoch 1003 loss: 0.055\n",
      "Epoch 1004 loss: 0.048\n",
      "Epoch 1005 loss: 0.059\n",
      "Epoch 1006 loss: 0.056\n",
      "Epoch 1007 loss: 0.048\n",
      "Epoch 1008 loss: 0.069\n",
      "Epoch 1009 loss: 0.056\n",
      "Epoch 1010 loss: 0.064\n",
      "Epoch 1011 loss: 0.067\n",
      "Epoch 1012 loss: 0.042\n",
      "Epoch 1013 loss: 0.046\n",
      "Epoch 1014 loss: 0.056\n",
      "Epoch 1015 loss: 0.072\n",
      "Epoch 1016 loss: 0.050\n",
      "Epoch 1017 loss: 0.051\n",
      "Epoch 1018 loss: 0.078\n",
      "Epoch 1019 loss: 0.052\n",
      "Epoch 1020 loss: 0.038\n",
      "Epoch 1021 loss: 0.045\n",
      "Epoch 1022 loss: 0.060\n",
      "Epoch 1023 loss: 0.053\n",
      "Epoch 1024 loss: 0.048\n",
      "Epoch 1025 loss: 0.059\n",
      "Epoch 1026 loss: 0.080\n",
      "Epoch 1027 loss: 0.071\n",
      "Epoch 1028 loss: 0.058\n",
      "Epoch 1029 loss: 0.060\n",
      "Epoch 1030 loss: 0.071\n",
      "Epoch 1031 loss: 0.053\n",
      "Epoch 1032 loss: 0.043\n",
      "Epoch 1033 loss: 0.042\n",
      "Epoch 1034 loss: 0.051\n",
      "Epoch 1035 loss: 0.041\n",
      "Epoch 1036 loss: 0.055\n",
      "Epoch 1037 loss: 0.047\n",
      "Epoch 1038 loss: 0.054\n",
      "Epoch 1039 loss: 0.049\n",
      "Epoch 1040 loss: 0.059\n",
      "Epoch 1041 loss: 0.083\n",
      "Epoch 1042 loss: 0.040\n",
      "Epoch 1043 loss: 0.049\n",
      "Epoch 1044 loss: 0.052\n",
      "Epoch 1045 loss: 0.066\n",
      "Epoch 1046 loss: 0.059\n",
      "Epoch 1047 loss: 0.038\n",
      "Epoch 1048 loss: 0.038\n",
      "Epoch 1049 loss: 0.036\n",
      "Epoch 1050 loss: 0.066\n",
      "Epoch 1051 loss: 0.074\n",
      "Epoch 1052 loss: 0.040\n",
      "Epoch 1053 loss: 0.050\n",
      "Epoch 1054 loss: 0.068\n",
      "Epoch 1055 loss: 0.069\n",
      "Epoch 1056 loss: 0.042\n",
      "Epoch 1057 loss: 0.062\n",
      "Epoch 1058 loss: 0.038\n",
      "Epoch 1059 loss: 0.080\n",
      "Epoch 1060 loss: 0.059\n",
      "Epoch 1061 loss: 0.067\n",
      "Epoch 1062 loss: 0.047\n",
      "Epoch 1063 loss: 0.054\n",
      "Epoch 1064 loss: 0.040\n",
      "Epoch 1065 loss: 0.056\n",
      "Epoch 1066 loss: 0.057\n",
      "Epoch 1067 loss: 0.056\n",
      "Epoch 1068 loss: 0.067\n",
      "Epoch 1069 loss: 0.037\n",
      "Epoch 1070 loss: 0.054\n",
      "Epoch 1071 loss: 0.048\n",
      "Epoch 1072 loss: 0.040\n",
      "Epoch 1073 loss: 0.076\n",
      "Epoch 1074 loss: 0.057\n",
      "Epoch 1075 loss: 0.033\n",
      "Epoch 1076 loss: 0.066\n",
      "Epoch 1077 loss: 0.061\n",
      "Epoch 1078 loss: 0.050\n",
      "Epoch 1079 loss: 0.064\n",
      "Epoch 1080 loss: 0.066\n",
      "Epoch 1081 loss: 0.047\n",
      "Epoch 1082 loss: 0.041\n",
      "Epoch 1083 loss: 0.067\n",
      "Epoch 1084 loss: 0.069\n",
      "Epoch 1085 loss: 0.062\n",
      "Epoch 1086 loss: 0.051\n",
      "Epoch 1087 loss: 0.031\n",
      "Epoch 1088 loss: 0.048\n",
      "Epoch 1089 loss: 0.052\n",
      "Epoch 1090 loss: 0.056\n",
      "Epoch 1091 loss: 0.048\n",
      "Epoch 1092 loss: 0.078\n",
      "Epoch 1093 loss: 0.049\n",
      "Epoch 1094 loss: 0.051\n",
      "Epoch 1095 loss: 0.054\n",
      "Epoch 1096 loss: 0.049\n",
      "Epoch 1097 loss: 0.062\n",
      "Epoch 1098 loss: 0.057\n",
      "Epoch 1099 loss: 0.054\n",
      "Epoch 1100 loss: 0.067\n",
      "Epoch 1101 loss: 0.056\n",
      "Epoch 1102 loss: 0.067\n",
      "Epoch 1103 loss: 0.074\n",
      "Epoch 1104 loss: 0.058\n",
      "Epoch 1105 loss: 0.064\n",
      "Epoch 1106 loss: 0.050\n",
      "Epoch 1107 loss: 0.056\n",
      "Epoch 1108 loss: 0.045\n",
      "Epoch 1109 loss: 0.037\n",
      "Epoch 1110 loss: 0.052\n",
      "Epoch 1111 loss: 0.057\n",
      "Epoch 1112 loss: 0.075\n",
      "Epoch 1113 loss: 0.054\n",
      "Epoch 1114 loss: 0.056\n",
      "Epoch 1115 loss: 0.058\n",
      "Epoch 1116 loss: 0.044\n",
      "Epoch 1117 loss: 0.068\n",
      "Epoch 1118 loss: 0.049\n",
      "Epoch 1119 loss: 0.057\n",
      "Epoch 1120 loss: 0.039\n",
      "Epoch 1121 loss: 0.067\n",
      "Epoch 1122 loss: 0.058\n",
      "Epoch 1123 loss: 0.064\n",
      "Epoch 1124 loss: 0.052\n",
      "Epoch 1125 loss: 0.041\n",
      "Epoch 1126 loss: 0.069\n",
      "Epoch 1127 loss: 0.062\n",
      "Epoch 1128 loss: 0.040\n",
      "Epoch 1129 loss: 0.051\n",
      "Epoch 1130 loss: 0.040\n",
      "Epoch 1131 loss: 0.074\n",
      "Epoch 1132 loss: 0.053\n",
      "Epoch 1133 loss: 0.042\n",
      "Epoch 1134 loss: 0.060\n",
      "Epoch 1135 loss: 0.045\n",
      "Epoch 1136 loss: 0.059\n",
      "Epoch 1137 loss: 0.069\n",
      "Epoch 1138 loss: 0.040\n",
      "Epoch 1139 loss: 0.048\n",
      "Epoch 1140 loss: 0.046\n",
      "Epoch 1141 loss: 0.046\n",
      "Epoch 1142 loss: 0.063\n",
      "Epoch 1143 loss: 0.040\n",
      "Epoch 1144 loss: 0.055\n",
      "Epoch 1145 loss: 0.059\n",
      "Epoch 1146 loss: 0.031\n",
      "Epoch 1147 loss: 0.044\n",
      "Epoch 1148 loss: 0.044\n",
      "Epoch 1149 loss: 0.050\n",
      "Epoch 1150 loss: 0.065\n",
      "Epoch 1151 loss: 0.057\n",
      "Epoch 1152 loss: 0.061\n",
      "Epoch 1153 loss: 0.049\n",
      "Epoch 1154 loss: 0.044\n",
      "Epoch 1155 loss: 0.054\n",
      "Epoch 1156 loss: 0.050\n",
      "Epoch 1157 loss: 0.041\n",
      "Epoch 1158 loss: 0.046\n",
      "Epoch 1159 loss: 0.057\n",
      "Epoch 1160 loss: 0.050\n",
      "Epoch 1161 loss: 0.048\n",
      "Epoch 1162 loss: 0.042\n",
      "Epoch 1163 loss: 0.055\n",
      "Epoch 1164 loss: 0.055\n",
      "Epoch 1165 loss: 0.037\n",
      "Epoch 1166 loss: 0.051\n",
      "Epoch 1167 loss: 0.067\n",
      "Epoch 1168 loss: 0.060\n",
      "Epoch 1169 loss: 0.044\n",
      "Epoch 1170 loss: 0.058\n",
      "Epoch 1171 loss: 0.054\n",
      "Epoch 1172 loss: 0.049\n",
      "Epoch 1173 loss: 0.047\n",
      "Epoch 1174 loss: 0.049\n",
      "Epoch 1175 loss: 0.055\n",
      "Epoch 1176 loss: 0.077\n",
      "Epoch 1177 loss: 0.062\n",
      "Epoch 1178 loss: 0.067\n",
      "Epoch 1179 loss: 0.044\n",
      "Epoch 1180 loss: 0.060\n",
      "Epoch 1181 loss: 0.077\n",
      "Epoch 1182 loss: 0.038\n",
      "Epoch 1183 loss: 0.060\n",
      "Epoch 1184 loss: 0.050\n",
      "Epoch 1185 loss: 0.061\n",
      "Epoch 1186 loss: 0.064\n",
      "Epoch 1187 loss: 0.057\n",
      "Epoch 1188 loss: 0.043\n",
      "Epoch 1189 loss: 0.045\n",
      "Epoch 1190 loss: 0.062\n",
      "Epoch 1191 loss: 0.065\n",
      "Epoch 1192 loss: 0.047\n",
      "Epoch 1193 loss: 0.065\n",
      "Epoch 1194 loss: 0.067\n",
      "Epoch 1195 loss: 0.062\n",
      "Epoch 1196 loss: 0.042\n",
      "Epoch 1197 loss: 0.063\n",
      "Epoch 1198 loss: 0.054\n",
      "Epoch 1199 loss: 0.040\n",
      "Epoch 1200 loss: 0.050\n",
      "Epoch 1201 loss: 0.054\n",
      "Epoch 1202 loss: 0.048\n",
      "Epoch 1203 loss: 0.071\n",
      "Epoch 1204 loss: 0.067\n",
      "Epoch 1205 loss: 0.044\n",
      "Epoch 1206 loss: 0.030\n",
      "Epoch 1207 loss: 0.049\n",
      "Epoch 1208 loss: 0.063\n",
      "Epoch 1209 loss: 0.051\n",
      "Epoch 1210 loss: 0.049\n",
      "Epoch 1211 loss: 0.068\n",
      "Epoch 1212 loss: 0.059\n",
      "Epoch 1213 loss: 0.047\n",
      "Epoch 1214 loss: 0.044\n",
      "Epoch 1215 loss: 0.026\n",
      "Epoch 1216 loss: 0.061\n",
      "Epoch 1217 loss: 0.047\n",
      "Epoch 1218 loss: 0.051\n",
      "Epoch 1219 loss: 0.052\n",
      "Epoch 1220 loss: 0.045\n",
      "Epoch 1221 loss: 0.030\n",
      "Epoch 1222 loss: 0.059\n",
      "Epoch 1223 loss: 0.085\n",
      "Epoch 1224 loss: 0.048\n",
      "Epoch 1225 loss: 0.057\n",
      "Epoch 1226 loss: 0.038\n",
      "Epoch 1227 loss: 0.041\n",
      "Epoch 1228 loss: 0.046\n",
      "Epoch 1229 loss: 0.040\n",
      "Epoch 1230 loss: 0.076\n",
      "Epoch 1231 loss: 0.060\n",
      "Epoch 1232 loss: 0.040\n",
      "Epoch 1233 loss: 0.061\n",
      "Epoch 1234 loss: 0.055\n",
      "Epoch 1235 loss: 0.065\n",
      "Epoch 1236 loss: 0.084\n",
      "Epoch 1237 loss: 0.060\n",
      "Epoch 1238 loss: 0.050\n",
      "Epoch 1239 loss: 0.060\n",
      "Epoch 1240 loss: 0.044\n",
      "Epoch 1241 loss: 0.038\n",
      "Epoch 1242 loss: 0.063\n",
      "Epoch 1243 loss: 0.050\n",
      "Epoch 1244 loss: 0.056\n",
      "Epoch 1245 loss: 0.048\n",
      "Epoch 1246 loss: 0.042\n",
      "Epoch 1247 loss: 0.054\n",
      "Epoch 1248 loss: 0.054\n",
      "Epoch 1249 loss: 0.054\n",
      "Epoch 1250 loss: 0.040\n",
      "Epoch 1251 loss: 0.043\n",
      "Epoch 1252 loss: 0.053\n",
      "Epoch 1253 loss: 0.082\n",
      "Epoch 1254 loss: 0.036\n",
      "Epoch 1255 loss: 0.059\n",
      "Epoch 1256 loss: 0.062\n",
      "Epoch 1257 loss: 0.049\n",
      "Epoch 1258 loss: 0.039\n",
      "Epoch 1259 loss: 0.066\n",
      "Epoch 1260 loss: 0.052\n",
      "Epoch 1261 loss: 0.040\n",
      "Epoch 1262 loss: 0.049\n",
      "Epoch 1263 loss: 0.049\n",
      "Epoch 1264 loss: 0.044\n",
      "Epoch 1265 loss: 0.057\n",
      "Epoch 1266 loss: 0.053\n",
      "Epoch 1267 loss: 0.049\n",
      "Epoch 1268 loss: 0.058\n",
      "Epoch 1269 loss: 0.053\n",
      "Epoch 1270 loss: 0.033\n",
      "Epoch 1271 loss: 0.053\n",
      "Epoch 1272 loss: 0.071\n",
      "Epoch 1273 loss: 0.053\n",
      "Epoch 1274 loss: 0.039\n",
      "Epoch 1275 loss: 0.047\n",
      "Epoch 1276 loss: 0.052\n",
      "Epoch 1277 loss: 0.057\n",
      "Epoch 1278 loss: 0.057\n",
      "Epoch 1279 loss: 0.041\n",
      "Epoch 1280 loss: 0.049\n",
      "Epoch 1281 loss: 0.041\n",
      "Epoch 1282 loss: 0.068\n",
      "Epoch 1283 loss: 0.063\n",
      "Epoch 1284 loss: 0.057\n",
      "Epoch 1285 loss: 0.063\n",
      "Epoch 1286 loss: 0.049\n",
      "Epoch 1287 loss: 0.034\n",
      "Epoch 1288 loss: 0.037\n",
      "Epoch 1289 loss: 0.056\n",
      "Epoch 1290 loss: 0.057\n",
      "Epoch 1291 loss: 0.032\n",
      "Epoch 1292 loss: 0.049\n",
      "Epoch 1293 loss: 0.050\n",
      "Epoch 1294 loss: 0.068\n",
      "Epoch 1295 loss: 0.055\n",
      "Epoch 1296 loss: 0.042\n",
      "Epoch 1297 loss: 0.048\n",
      "Epoch 1298 loss: 0.050\n",
      "Epoch 1299 loss: 0.036\n",
      "Epoch 1300 loss: 0.041\n",
      "Epoch 1301 loss: 0.050\n",
      "Epoch 1302 loss: 0.063\n",
      "Epoch 1303 loss: 0.049\n",
      "Epoch 1304 loss: 0.050\n",
      "Epoch 1305 loss: 0.045\n",
      "Epoch 1306 loss: 0.061\n",
      "Epoch 1307 loss: 0.036\n",
      "Epoch 1308 loss: 0.050\n",
      "Epoch 1309 loss: 0.061\n",
      "Epoch 1310 loss: 0.050\n",
      "Epoch 1311 loss: 0.062\n",
      "Epoch 1312 loss: 0.035\n",
      "Epoch 1313 loss: 0.043\n",
      "Epoch 1314 loss: 0.043\n",
      "Epoch 1315 loss: 0.071\n",
      "Epoch 1316 loss: 0.063\n",
      "Epoch 1317 loss: 0.067\n",
      "Epoch 1318 loss: 0.042\n",
      "Epoch 1319 loss: 0.039\n",
      "Epoch 1320 loss: 0.054\n",
      "Epoch 1321 loss: 0.045\n",
      "Epoch 1322 loss: 0.063\n",
      "Epoch 1323 loss: 0.048\n",
      "Epoch 1324 loss: 0.059\n",
      "Epoch 1325 loss: 0.069\n",
      "Epoch 1326 loss: 0.059\n",
      "Epoch 1327 loss: 0.059\n",
      "Epoch 1328 loss: 0.080\n",
      "Epoch 1329 loss: 0.057\n",
      "Epoch 1330 loss: 0.072\n",
      "Epoch 1331 loss: 0.046\n",
      "Epoch 1332 loss: 0.043\n",
      "Epoch 1333 loss: 0.056\n",
      "Epoch 1334 loss: 0.060\n",
      "Epoch 1335 loss: 0.032\n",
      "Epoch 1336 loss: 0.041\n",
      "Epoch 1337 loss: 0.069\n",
      "Epoch 1338 loss: 0.050\n",
      "Epoch 1339 loss: 0.067\n",
      "Epoch 1340 loss: 0.055\n",
      "Epoch 1341 loss: 0.047\n",
      "Epoch 1342 loss: 0.055\n",
      "Epoch 1343 loss: 0.068\n",
      "Epoch 1344 loss: 0.051\n",
      "Epoch 1345 loss: 0.045\n",
      "Epoch 1346 loss: 0.037\n",
      "Epoch 1347 loss: 0.056\n",
      "Epoch 1348 loss: 0.057\n",
      "Epoch 1349 loss: 0.036\n",
      "Epoch 1350 loss: 0.058\n",
      "Epoch 1351 loss: 0.045\n",
      "Epoch 1352 loss: 0.031\n",
      "Epoch 1353 loss: 0.065\n",
      "Epoch 1354 loss: 0.039\n",
      "Epoch 1355 loss: 0.058\n",
      "Epoch 1356 loss: 0.057\n",
      "Epoch 1357 loss: 0.037\n",
      "Epoch 1358 loss: 0.073\n",
      "Epoch 1359 loss: 0.050\n",
      "Epoch 1360 loss: 0.050\n",
      "Epoch 1361 loss: 0.054\n",
      "Epoch 1362 loss: 0.064\n",
      "Epoch 1363 loss: 0.050\n",
      "Epoch 1364 loss: 0.057\n",
      "Epoch 1365 loss: 0.030\n",
      "Epoch 1366 loss: 0.037\n",
      "Epoch 1367 loss: 0.032\n",
      "Epoch 1368 loss: 0.062\n",
      "Epoch 1369 loss: 0.034\n",
      "Epoch 1370 loss: 0.061\n",
      "Epoch 1371 loss: 0.050\n",
      "Epoch 1372 loss: 0.036\n",
      "Epoch 1373 loss: 0.041\n",
      "Epoch 1374 loss: 0.044\n",
      "Epoch 1375 loss: 0.053\n",
      "Epoch 1376 loss: 0.065\n",
      "Epoch 1377 loss: 0.036\n",
      "Epoch 1378 loss: 0.058\n",
      "Epoch 1379 loss: 0.050\n",
      "Epoch 1380 loss: 0.045\n",
      "Epoch 1381 loss: 0.062\n",
      "Epoch 1382 loss: 0.074\n",
      "Epoch 1383 loss: 0.047\n",
      "Epoch 1384 loss: 0.038\n",
      "Epoch 1385 loss: 0.049\n",
      "Epoch 1386 loss: 0.038\n",
      "Epoch 1387 loss: 0.068\n",
      "Epoch 1388 loss: 0.064\n",
      "Epoch 1389 loss: 0.051\n",
      "Epoch 1390 loss: 0.071\n",
      "Epoch 1391 loss: 0.064\n",
      "Epoch 1392 loss: 0.057\n",
      "Epoch 1393 loss: 0.046\n",
      "Epoch 1394 loss: 0.068\n",
      "Epoch 1395 loss: 0.040\n",
      "Epoch 1396 loss: 0.052\n",
      "Epoch 1397 loss: 0.054\n",
      "Epoch 1398 loss: 0.053\n",
      "Epoch 1399 loss: 0.042\n",
      "Epoch 1400 loss: 0.034\n",
      "Epoch 1401 loss: 0.064\n",
      "Epoch 1402 loss: 0.030\n",
      "Epoch 1403 loss: 0.047\n",
      "Epoch 1404 loss: 0.057\n",
      "Epoch 1405 loss: 0.019\n",
      "Epoch 1406 loss: 0.060\n",
      "Epoch 1407 loss: 0.043\n",
      "Epoch 1408 loss: 0.061\n",
      "Epoch 1409 loss: 0.055\n",
      "Epoch 1410 loss: 0.048\n",
      "Epoch 1411 loss: 0.071\n",
      "Epoch 1412 loss: 0.054\n",
      "Epoch 1413 loss: 0.029\n",
      "Epoch 1414 loss: 0.061\n",
      "Epoch 1415 loss: 0.051\n",
      "Epoch 1416 loss: 0.041\n",
      "Epoch 1417 loss: 0.061\n",
      "Epoch 1418 loss: 0.059\n",
      "Epoch 1419 loss: 0.042\n",
      "Epoch 1420 loss: 0.062\n",
      "Epoch 1421 loss: 0.043\n",
      "Epoch 1422 loss: 0.035\n",
      "Epoch 1423 loss: 0.064\n",
      "Epoch 1424 loss: 0.034\n",
      "Epoch 1425 loss: 0.056\n",
      "Epoch 1426 loss: 0.061\n",
      "Epoch 1427 loss: 0.062\n",
      "Epoch 1428 loss: 0.061\n",
      "Epoch 1429 loss: 0.054\n",
      "Epoch 1430 loss: 0.048\n",
      "Epoch 1431 loss: 0.048\n",
      "Epoch 1432 loss: 0.073\n",
      "Epoch 1433 loss: 0.051\n",
      "Epoch 1434 loss: 0.032\n",
      "Epoch 1435 loss: 0.038\n",
      "Epoch 1436 loss: 0.040\n",
      "Epoch 1437 loss: 0.045\n",
      "Epoch 1438 loss: 0.050\n",
      "Epoch 1439 loss: 0.037\n",
      "Epoch 1440 loss: 0.052\n",
      "Epoch 1441 loss: 0.063\n",
      "Epoch 1442 loss: 0.060\n",
      "Epoch 1443 loss: 0.052\n",
      "Epoch 1444 loss: 0.049\n",
      "Epoch 1445 loss: 0.040\n",
      "Epoch 1446 loss: 0.039\n",
      "Epoch 1447 loss: 0.043\n",
      "Epoch 1448 loss: 0.038\n",
      "Epoch 1449 loss: 0.055\n",
      "Epoch 1450 loss: 0.027\n",
      "Epoch 1451 loss: 0.057\n",
      "Epoch 1452 loss: 0.058\n",
      "Epoch 1453 loss: 0.063\n",
      "Epoch 1454 loss: 0.037\n",
      "Epoch 1455 loss: 0.048\n",
      "Epoch 1456 loss: 0.041\n",
      "Epoch 1457 loss: 0.034\n",
      "Epoch 1458 loss: 0.049\n",
      "Epoch 1459 loss: 0.052\n",
      "Epoch 1460 loss: 0.070\n",
      "Epoch 1461 loss: 0.035\n",
      "Epoch 1462 loss: 0.065\n",
      "Epoch 1463 loss: 0.055\n",
      "Epoch 1464 loss: 0.054\n",
      "Epoch 1465 loss: 0.045\n",
      "Epoch 1466 loss: 0.051\n",
      "Epoch 1467 loss: 0.080\n",
      "Epoch 1468 loss: 0.075\n",
      "Epoch 1469 loss: 0.049\n",
      "Epoch 1470 loss: 0.032\n",
      "Epoch 1471 loss: 0.062\n",
      "Epoch 1472 loss: 0.070\n",
      "Epoch 1473 loss: 0.045\n",
      "Epoch 1474 loss: 0.039\n",
      "Epoch 1475 loss: 0.053\n",
      "Epoch 1476 loss: 0.044\n",
      "Epoch 1477 loss: 0.045\n",
      "Epoch 1478 loss: 0.058\n",
      "Epoch 1479 loss: 0.035\n",
      "Epoch 1480 loss: 0.064\n",
      "Epoch 1481 loss: 0.046\n",
      "Epoch 1482 loss: 0.051\n",
      "Epoch 1483 loss: 0.076\n",
      "Epoch 1484 loss: 0.053\n",
      "Epoch 1485 loss: 0.042\n",
      "Epoch 1486 loss: 0.036\n",
      "Epoch 1487 loss: 0.063\n",
      "Epoch 1488 loss: 0.042\n",
      "Epoch 1489 loss: 0.062\n",
      "Epoch 1490 loss: 0.054\n",
      "Epoch 1491 loss: 0.039\n",
      "Epoch 1492 loss: 0.037\n",
      "Epoch 1493 loss: 0.050\n",
      "Epoch 1494 loss: 0.033\n",
      "Epoch 1495 loss: 0.040\n",
      "Epoch 1496 loss: 0.047\n",
      "Epoch 1497 loss: 0.035\n",
      "Epoch 1498 loss: 0.046\n",
      "Epoch 1499 loss: 0.053\n",
      "Epoch 1500 loss: 0.047\n"
     ]
    }
   ],
   "source": [
    "groups = torch.tensor([0,0,0,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "# Define data views\n",
    "views = [torch.tensor(rna_16s.T.values, dtype = torch.float32),\n",
    "        torch.tensor(metaproteomics.T.values, dtype = torch.float32),\n",
    "        torch.tensor(metabpos.T.values, dtype = torch.float32),\n",
    "        torch.tensor(metabneg.T.values, dtype = torch.float32)]\n",
    "\n",
    "acc_mlp = []\n",
    "\n",
    "# Time: 20 seconds             \n",
    "for i in range(1500):\n",
    "\n",
    "    # Update the mlp\n",
    "    yhat, h, yhats, hiddens = joint_mlp(*views)\n",
    "\n",
    "    # pass the predictions and distributions to the loss function and update parameters\n",
    "    _, _, loss = joint_mlp.loss(groups, yhat, yhats)\n",
    "\n",
    "    optimizer_mlp.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(joint_mlp.parameters(), 2.0)\n",
    "    optimizer_mlp.step()\n",
    "\n",
    "    print(f'Epoch {i+1} loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointMLP(\n",
       "  (margin_models): ModuleList(\n",
       "    (0): simple_FC(\n",
       "      (fc1): Linear(in_features=8, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=64, bias=True)\n",
       "      (fc_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1): simple_FC(\n",
       "      (fc1): Linear(in_features=2726, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=64, bias=True)\n",
       "      (fc_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (2): simple_FC(\n",
       "      (fc1): Linear(in_features=2800, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=64, bias=True)\n",
       "      (fc_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (3): simple_FC(\n",
       "      (fc1): Linear(in_features=1752, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=64, bias=True)\n",
       "      (fc_out): Linear(in_features=64, out_features=2, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: margin_models.0.fc1.weight, Size: torch.Size([16, 8])\n",
      "Layer: margin_models.0.fc2.weight, Size: torch.Size([64, 16])\n",
      "Layer: margin_models.0.fc_out.weight, Size: torch.Size([2, 64])\n",
      "Layer: margin_models.1.fc1.weight, Size: torch.Size([4096, 2726])\n",
      "Layer: margin_models.1.fc2.weight, Size: torch.Size([64, 4096])\n",
      "Layer: margin_models.1.fc_out.weight, Size: torch.Size([2, 64])\n",
      "Layer: margin_models.2.fc1.weight, Size: torch.Size([4096, 2800])\n",
      "Layer: margin_models.2.fc2.weight, Size: torch.Size([64, 4096])\n",
      "Layer: margin_models.2.fc_out.weight, Size: torch.Size([2, 64])\n",
      "Layer: margin_models.3.fc1.weight, Size: torch.Size([4096, 1752])\n",
      "Layer: margin_models.3.fc2.weight, Size: torch.Size([64, 4096])\n",
      "Layer: margin_models.3.fc_out.weight, Size: torch.Size([2, 64])\n",
      "Layer: fc1.weight, Size: torch.Size([64, 64])\n",
      "Layer: fc2.weight, Size: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "for name, param in joint_mlp.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"Layer: {name}, Size: {param.data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2790,  0.2309, -0.0063, -0.2579,  0.0850, -0.3016,  0.0200, -0.0869],\n",
       "        [-0.2498, -0.1668, -0.0803, -0.3449,  0.2276,  0.3328,  0.1798,  0.3152],\n",
       "        [-0.0541,  0.0271, -0.1936,  0.2319,  0.2634, -0.3089,  0.1669,  0.2696],\n",
       "        [-0.3063, -0.0737, -0.0033, -0.0562, -0.1863,  0.1694, -0.2140,  0.3235],\n",
       "        [ 0.1182, -0.3317, -0.1386,  0.1005, -0.0682,  0.2948,  0.2535,  0.2093],\n",
       "        [-0.1675,  0.0162, -0.1980,  0.2635, -0.1848,  0.3976,  0.1379, -0.0448],\n",
       "        [ 0.1726, -0.0071, -0.3636, -0.1580, -0.2271, -0.0712, -0.0891,  0.0193],\n",
       "        [ 0.2241,  0.2741,  0.1876,  0.2943, -0.2548,  0.2008, -0.1256, -0.2199],\n",
       "        [-0.0502, -0.3924,  0.2249, -0.1255, -0.3172,  0.1670,  0.0772,  0.1702],\n",
       "        [-0.1431, -0.0323, -0.3135,  0.3107, -0.1539,  0.2071, -0.1078,  0.1275],\n",
       "        [-0.0265, -0.0756,  0.1487,  0.2065, -0.4041,  0.0938,  0.2345,  0.0952],\n",
       "        [ 0.3062, -0.2431,  0.1785, -0.3100,  0.3237,  0.1039,  0.1919, -0.2243],\n",
       "        [ 0.0654,  0.2814, -0.3630,  0.0813, -0.2852,  0.0786,  0.2052, -0.2811],\n",
       "        [ 0.2994,  0.1783,  0.3284, -0.2433, -0.1163, -0.2308, -0.0682, -0.1937],\n",
       "        [-0.2712,  0.1729,  0.2444, -0.0313, -0.1494,  0.2270,  0.1348, -0.2553],\n",
       "        [-0.1083,  0.2356, -0.3225,  0.0075, -0.2874,  0.3168,  0.0460, -0.0008]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V Matrix (V1)\n",
    "joint_mlp.margin_models[0].fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2131, -0.2432, -0.0552,  ...,  0.2356, -0.1717, -0.0754],\n",
       "        [ 0.2080, -0.2044,  0.1308,  ..., -0.0826,  0.0228, -0.1007],\n",
       "        [-0.1071,  0.0178, -0.0762,  ..., -0.1366, -0.2381, -0.0738],\n",
       "        ...,\n",
       "        [ 0.1545,  0.2002, -0.1474,  ...,  0.1817, -0.1117,  0.2896],\n",
       "        [ 0.1910,  0.2340, -0.1562,  ...,  0.0482,  0.0609,  0.0120],\n",
       "        [-0.0745,  0.1786,  0.2316,  ..., -0.1642, -0.1805, -0.2076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights for Features in V1 (V2)\n",
    "joint_mlp.margin_models[0].fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1288, -0.1215, -0.0963,  0.0599,  0.0039,  0.0739, -0.0406,  0.0122,\n",
       "          0.1709,  0.0703, -0.0570, -0.1277,  0.0548,  0.0993, -0.0809,  0.0239,\n",
       "         -0.0780,  0.0338, -0.1634, -0.0497,  0.1177,  0.0447, -0.0412, -0.1204,\n",
       "         -0.1221, -0.0976,  0.0885,  0.0990,  0.0912,  0.0005,  0.0464, -0.1301,\n",
       "          0.0812, -0.0997, -0.0258,  0.0805, -0.1269,  0.0561, -0.1737, -0.0667,\n",
       "          0.0629, -0.1432,  0.1169, -0.0914, -0.0677, -0.0933, -0.0600, -0.0720,\n",
       "          0.0337,  0.0050, -0.0960, -0.1327, -0.0326, -0.0997,  0.1280, -0.0814,\n",
       "         -0.0429, -0.0659, -0.0628, -0.1549,  0.0058, -0.0214, -0.1393, -0.0646],\n",
       "        [ 0.0765,  0.0541, -0.0277, -0.1206,  0.1063, -0.0142, -0.0745,  0.1606,\n",
       "         -0.1578,  0.0716,  0.1374,  0.1087, -0.0935, -0.1224, -0.0281, -0.0026,\n",
       "          0.1334, -0.0084,  0.0819,  0.0893, -0.0786,  0.0533, -0.0921,  0.0290,\n",
       "          0.1141, -0.0405, -0.0929, -0.1405, -0.0682,  0.0143,  0.0282, -0.0307,\n",
       "         -0.1227,  0.1108, -0.0199,  0.0556,  0.0564, -0.0189,  0.1456,  0.0579,\n",
       "         -0.0576, -0.0278, -0.0756, -0.0047, -0.0784,  0.1510,  0.1167,  0.1274,\n",
       "          0.0211, -0.0680, -0.0451,  0.0967,  0.1430,  0.0991, -0.0852,  0.1152,\n",
       "         -0.0930, -0.0496, -0.0728,  0.0723, -0.0488, -0.0636,  0.1010, -0.0994]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each V2 weight for its class [0 or 1] (V3)\n",
    "joint_mlp.margin_models[0].fc_out.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1054,  0.0732, -0.0925, -0.1017,  0.1042, -0.0540, -0.0952, -0.0391,\n",
       "          0.0752, -0.0404,  0.0474, -0.0726, -0.1169,  0.0741, -0.0197,  0.0486,\n",
       "          0.0855, -0.0506,  0.0540, -0.0649,  0.0024, -0.0510,  0.0341, -0.0650,\n",
       "          0.0863,  0.0556, -0.0969,  0.1085,  0.0249, -0.0529, -0.1295,  0.0832,\n",
       "          0.0008, -0.0110,  0.1011,  0.0792, -0.0973, -0.1034,  0.0873,  0.0810,\n",
       "         -0.1242, -0.0622, -0.0250, -0.0249,  0.0772,  0.0910,  0.0708, -0.0841,\n",
       "          0.0697,  0.0870,  0.1200,  0.0933,  0.0778, -0.0024,  0.0644,  0.0394,\n",
       "         -0.1050, -0.0313,  0.1104,  0.0147,  0.0391, -0.1236, -0.0438, -0.1183],\n",
       "        [-0.0874,  0.0761,  0.0955,  0.0800, -0.0843, -0.0491, -0.0430, -0.0830,\n",
       "         -0.0780,  0.0260, -0.0871, -0.0731, -0.0145,  0.0468,  0.1188,  0.0956,\n",
       "         -0.0449,  0.0463,  0.1070, -0.0012, -0.1100,  0.0723,  0.0003, -0.0532,\n",
       "          0.0298,  0.0102, -0.0806, -0.0366,  0.0241,  0.0113, -0.0301,  0.0929,\n",
       "          0.0255, -0.0894, -0.0072, -0.0569, -0.0497,  0.0427,  0.1086, -0.0621,\n",
       "         -0.0548,  0.0303,  0.0821, -0.0040,  0.1256,  0.0198, -0.1061, -0.0627,\n",
       "         -0.0853, -0.1187,  0.0776, -0.0531,  0.0253,  0.0139,  0.1115, -0.0468,\n",
       "         -0.1262,  0.0812, -0.0210, -0.0829, -0.0805,  0.1209, -0.0374, -0.0848]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_mlp.margin_models[2].fc_out.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# need to wrap the model in this class to get around some issues with the SHAP package\n",
    "class JointMLPWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    " \n",
    "    def forward(self, *datas):\n",
    "        yhat, _, _, _ = self.model(*datas)\n",
    "        return yhat\n",
    "    \n",
    "joint_model_wrp = JointMLPWrapper(joint_mlp)\n",
    "\n",
    "# make the explainer object\n",
    "explainer = shap.DeepExplainer(joint_model_wrp, views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(views, check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Shapley Value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "View",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "191de8f3-7b56-4cfb-b7f3-5c7dc20c4112",
       "rows": [
        [
         "0",
         "-1.5020112954289289e-05",
         "Variovorax",
         "16S"
        ],
        [
         "1",
         "-3.0935628956285655e-05",
         "Sphingopyxis",
         "16S"
        ],
        [
         "2",
         "-9.96741894709885e-06",
         "Ensifer",
         "16S"
        ],
        [
         "3",
         "-2.631961962151763e-06",
         "Rhodococcus",
         "16S"
        ],
        [
         "4",
         "-5.4669278028995905e-05",
         "Dyadobacter",
         "16S"
        ],
        [
         "5",
         "-5.3615390015693265e-05",
         "Neorhizobium",
         "16S"
        ],
        [
         "6",
         "-7.454951742147387e-05",
         "Streptomyces",
         "16S"
        ],
        [
         "7",
         "-7.148932059664048e-07",
         "Sinorhizobium",
         "16S"
        ],
        [
         "8",
         "-8.739810425595351e-06",
         "G1_CDS.1016",
         "metaproteomics"
        ],
        [
         "9",
         "-5.056764607047626e-06",
         "G1_CDS.1017",
         "metaproteomics"
        ],
        [
         "10",
         "-1.0910905189120967e-05",
         "G1_CDS.1022",
         "metaproteomics"
        ],
        [
         "11",
         "-6.59417341886126e-06",
         "G1_CDS.1023",
         "metaproteomics"
        ],
        [
         "12",
         "-1.2966150734428084e-05",
         "G1_CDS.1025",
         "metaproteomics"
        ],
        [
         "13",
         "-3.2048359367431445e-06",
         "G1_CDS.1032",
         "metaproteomics"
        ],
        [
         "14",
         "-4.2839182157194955e-06",
         "G1_CDS.1034",
         "metaproteomics"
        ],
        [
         "15",
         "-1.2365753011778224e-05",
         "G1_CDS.1035",
         "metaproteomics"
        ],
        [
         "16",
         "-9.052522557340126e-06",
         "G1_CDS.1036",
         "metaproteomics"
        ],
        [
         "17",
         "-1.2995200165732967e-05",
         "G1_CDS.1038",
         "metaproteomics"
        ],
        [
         "18",
         "-1.7227403894537474e-06",
         "G1_CDS.1039",
         "metaproteomics"
        ],
        [
         "19",
         "-2.550879187879218e-07",
         "G1_CDS.1044",
         "metaproteomics"
        ],
        [
         "20",
         "-1.9557146686111082e-05",
         "G1_CDS.1045",
         "metaproteomics"
        ],
        [
         "21",
         "-8.696271081021223e-06",
         "G1_CDS.1048",
         "metaproteomics"
        ],
        [
         "22",
         "-9.792227103844198e-06",
         "G1_CDS.1049",
         "metaproteomics"
        ],
        [
         "23",
         "-2.3972310874142977e-06",
         "G1_CDS.1050",
         "metaproteomics"
        ],
        [
         "24",
         "-4.0755586105944985e-06",
         "G1_CDS.1051",
         "metaproteomics"
        ],
        [
         "25",
         "-7.313492247362774e-06",
         "G1_CDS.1055",
         "metaproteomics"
        ],
        [
         "26",
         "-7.5563546886314725e-06",
         "G1_CDS.1056",
         "metaproteomics"
        ],
        [
         "27",
         "-7.072810802810636e-06",
         "G1_CDS.1071",
         "metaproteomics"
        ],
        [
         "28",
         "-1.2028310152345512e-06",
         "G1_CDS.1078",
         "metaproteomics"
        ],
        [
         "29",
         "-8.355246706059916e-06",
         "G1_CDS.1084",
         "metaproteomics"
        ],
        [
         "30",
         "-1.232699872844023e-05",
         "G1_CDS.1101",
         "metaproteomics"
        ],
        [
         "31",
         "-8.85989706489454e-06",
         "G1_CDS.1110",
         "metaproteomics"
        ],
        [
         "32",
         "-8.040580354418125e-06",
         "G1_CDS.1148",
         "metaproteomics"
        ],
        [
         "33",
         "-6.082241537086475e-06",
         "G1_CDS.1168",
         "metaproteomics"
        ],
        [
         "34",
         "-1.3931546831713604e-06",
         "G1_CDS.1173",
         "metaproteomics"
        ],
        [
         "35",
         "-6.247714892992917e-06",
         "G1_CDS.1174",
         "metaproteomics"
        ],
        [
         "36",
         "-8.793614085789159e-06",
         "G1_CDS.1175",
         "metaproteomics"
        ],
        [
         "37",
         "-7.111208491394905e-06",
         "G1_CDS.1337",
         "metaproteomics"
        ],
        [
         "38",
         "-1.3158754939013306e-05",
         "G1_CDS.1341",
         "metaproteomics"
        ],
        [
         "39",
         "-7.380515512522834e-06",
         "G1_CDS.1354",
         "metaproteomics"
        ],
        [
         "40",
         "-8.357000837122541e-06",
         "G1_CDS.1381",
         "metaproteomics"
        ],
        [
         "41",
         "4.749960711564194e-08",
         "G1_CDS.1385",
         "metaproteomics"
        ],
        [
         "42",
         "-2.2453166970137772e-06",
         "G1_CDS.1393",
         "metaproteomics"
        ],
        [
         "43",
         "-1.1778429211517505e-05",
         "G1_CDS.1420",
         "metaproteomics"
        ],
        [
         "44",
         "-1.5466428351373906e-05",
         "G1_CDS.149",
         "metaproteomics"
        ],
        [
         "45",
         "-5.914485583957685e-06",
         "G1_CDS.153",
         "metaproteomics"
        ],
        [
         "46",
         "-1.8855658311167645e-05",
         "G1_CDS.154",
         "metaproteomics"
        ],
        [
         "47",
         "-7.993502963987709e-07",
         "G1_CDS.155",
         "metaproteomics"
        ],
        [
         "48",
         "-9.466496578625083e-06",
         "G1_CDS.156",
         "metaproteomics"
        ],
        [
         "49",
         "-2.863876019887357e-06",
         "G1_CDS.157",
         "metaproteomics"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 7286
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shapley Value</th>\n",
       "      <th>Feature</th>\n",
       "      <th>View</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.502011e-05</td>\n",
       "      <td>Variovorax</td>\n",
       "      <td>16S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.093563e-05</td>\n",
       "      <td>Sphingopyxis</td>\n",
       "      <td>16S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9.967419e-06</td>\n",
       "      <td>Ensifer</td>\n",
       "      <td>16S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.631962e-06</td>\n",
       "      <td>Rhodococcus</td>\n",
       "      <td>16S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.466928e-05</td>\n",
       "      <td>Dyadobacter</td>\n",
       "      <td>16S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7281</th>\n",
       "      <td>-7.611933e-06</td>\n",
       "      <td>4.95_213.12411</td>\n",
       "      <td>metabolomics negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7282</th>\n",
       "      <td>-4.591290e-07</td>\n",
       "      <td>1.17_330.841266</td>\n",
       "      <td>metabolomics negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7283</th>\n",
       "      <td>-1.815025e-06</td>\n",
       "      <td>7.15_303.123453</td>\n",
       "      <td>metabolomics negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7284</th>\n",
       "      <td>-1.290065e-05</td>\n",
       "      <td>5.78_226.035199</td>\n",
       "      <td>metabolomics negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7285</th>\n",
       "      <td>-1.139606e-05</td>\n",
       "      <td>1.01_459.138407</td>\n",
       "      <td>metabolomics negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7286 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Shapley Value          Feature                   View\n",
       "0     -1.502011e-05       Variovorax                    16S\n",
       "1     -3.093563e-05     Sphingopyxis                    16S\n",
       "2     -9.967419e-06          Ensifer                    16S\n",
       "3     -2.631962e-06      Rhodococcus                    16S\n",
       "4     -5.466928e-05      Dyadobacter                    16S\n",
       "...             ...              ...                    ...\n",
       "7281  -7.611933e-06   4.95_213.12411  metabolomics negative\n",
       "7282  -4.591290e-07  1.17_330.841266  metabolomics negative\n",
       "7283  -1.815025e-06  7.15_303.123453  metabolomics negative\n",
       "7284  -1.290065e-05  5.78_226.035199  metabolomics negative\n",
       "7285  -1.139606e-05  1.01_459.138407  metabolomics negative\n",
       "\n",
       "[7286 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First index is which class (00wk or post-00wk). The second index is which view. The third index is the shapley values by samples per feature.\n",
    "# Shapley values were summed across samples. \n",
    "\n",
    "shap_vals = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        \"Shapley Value\": pd.DataFrame(shap_values[0][0]).sum().tolist(),\n",
    "        \"Feature\": pd.read_csv(\"../../Dataset/Model_Ready/16S_Edata.csv\")[\"Feature\"].tolist(),\n",
    "        \"View\": [\"16S\" for x in range(len(rna_16s))]\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"Shapley Value\": pd.DataFrame(shap_values[0][1]).sum().tolist(),\n",
    "        \"Feature\": pd.read_csv(\"../../Dataset/Model_Ready/Metaproteomics.csv\")[\"Feature\"].tolist(),\n",
    "        \"View\": [\"metaproteomics\" for x in range(len(metaproteomics))]\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"Shapley Value\": pd.DataFrame(shap_values[0][2]).sum().tolist(),\n",
    "        \"Feature\": pd.read_csv(\"../../Dataset/Model_Ready/Metabolomics_Positive.csv\")[\"Feature\"].tolist(),\n",
    "        \"View\": [\"metabolomics positive\" for x in range(len(metabpos))]\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        \"Shapley Value\": pd.DataFrame(shap_values[0][3]).sum().tolist(),\n",
    "        \"Feature\": pd.read_csv(\"../../Dataset/Model_Ready/Metabolomics_Negative.csv\")[\"Feature\"].tolist(),\n",
    "        \"View\": [\"metabolomics negative\" for x in range(len(metabneg))]\n",
    "    }),\n",
    "]).reset_index(drop = True)\n",
    "\n",
    "shap_vals.to_csv(\"DeepIMV_ShapValues.csv\", index = False)\n",
    "\n",
    "shap_vals\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_imv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
